\chapter{基于卷积神经网络的贪婪式概率标注算法}
\label{chap:chap03}
本章首先给出了基于卷积神经网络的贪婪式概率标注算法(简称为 $CNN-IETS$ )概述，然后给出了分割式信息抽取的正式定义，并介绍了知识库的结构。接下来详细介绍了算法每一部分的内容，最后给出了详细的实验结果来证明算法的效果和表现。


\section{算法概述}

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{../figures/chap03/framework.pdf}\\
  \smallcaption{模型的整体结构}
  \label{chap:chap03:fig:framework}
\end{figure}

这一章，我们提出了一种非监督式概率标注算法，它基于深度卷积神经网络(CNN)来构建模型，借助提前构造的知识库来克服需要人工标注语料的难题。卷积神经网络是一种端到端的模型，先天具有抽取特征和组合特征的优势，使用深度卷积神经网络能够自动得选择出高质量的特征来更好的刻画各个属性值。而这个知识库有两个作用，第一个作用是用来训练我们的卷积神经网络模型分类器，第二个作用是协助生成文本段，再输入到我们的分类器中。具体一点讲，如图~\ref{chap:chap03:fig:framework}，输入一个文本,我们首先根据词语在知识库中的共现来初始分隔这条文本，形成若干个文本段。对于这些文本段，如果它在某个属性组中出现的频率比较高而在其他属性组中出现的频率比较低，并且得分高于某个阈值，它就会被标注成这个属性。然后根据这些初始的分割状态和标注结果，哪些没有标注的文本段会被输入到下一步的CNN分类器中。在下一步，我们并不是直接接受CNN分类器的输出结果，我们根据CNN分类器的输出概率发明了一种\emph{贪婪式概率标注算法}，这个算法会从全局考虑分割和标注状态来为输入文本执行最合理的分割和标注方式。对于一些有上面过程产生的漏标注和错误标注的情形，我们提出了一种\emph{双向位置与序列模型}，进一步对标注结果进行调整和改善。

为了构建我们的卷积神经网络模型，我们使用了一组全面的语义特征和句法特征加入到模型的输入中，这些特征都是直接从我们的知识库中构造的，这样我们的模型就可以充分利用这些特征来标注每一个独立的文本段。虽然，我们的模型可以直接提供一个标注的结果，但是这些结果仅仅是面对单独的文本段，没有考虑在同一个输入文本中各文本段之间的内在关系，这也不能找到最佳的分割和标注结果。因此，我们需要从全局考虑分割和标注状态来执行概率标注。因此，最理想的标注方式是，我们依次标注每个文本块来最大化所有标注结果的概率和，但在这个过程中，必须满足一个前提条件，即每个属性值只能包含连续的文本段。我们首先证明了这是一个NP-hard的问题，然后提出了一种贪婪式概率标注算法，此算法能够有效的实现最佳标注方式。

使用上述的基于卷积神经网络的贪婪式概率标注算法，大部分的输入文本都能够被正确分割和标注了，但是还是会出现一些漏标注和错误标注问题。为了解决此类问题，我们在执行抽取的过程中了权衡考虑了已经标注文本段的位置和序列信息，相比较文章\ucite{cortez2010ondux}中提出的单向序列模型，我们的是双向模型，从前向和后向同时考虑序列关系，因此能够减少更多的标注错误并提供更大的帮助。

\section{基于卷积神经网络的贪婪式概率标注算法}
在这一节，我们首先给出分割式信息抽取任务的定义，然后详细介绍借助知识库我卷积神经网络构建的贪婪式概率标注算法(CNN-IETS)。

\begin{definition}
{\bf (分割式信息抽取任务)}
对于某个领域的属性集 $\mathbb{A}$, 假设输入文本 $I$ 包含了一些属性集 $\mathbb{A}$中的属性值, 分割式信息抽取任务目的是分割 $I$ 成一些列的文本段 $S=\{s_1, s_2, ..., s_m\}$, 然后使用一个属性值 $A_i \in \mathbb{A}$ 来标注每一个文本段 $s_i$ ($1\leq i \leq m$), 这里 $s_i$ 数属性 $A_i$中的一个属性值.
\end{definition}

\noindent \underline{\em 知识库:}
我们使用实现存在的语料库来构建一个固定领域的知识库(简称为KB)，就像\cite{cortez2010ondux, zhao2008exploiting}中做的一样。简单讲，知识库(KB)包含一组特定的属性以及对应的属性值，我们记做 KB=$\{ <A_1, V_1>, <A_2, V_2>, ..., <A_n, V_n> \}$ ，这里每一个属性$A_i$ ($1\leq i \leq n$)都是这个领域属性集  $\mathbb{A}$ 中的一个独立属性， $V_i$ 是一组词语，是在属性$a_i$中出现的典型的、合理的值，并且记录每个属性值在记录中的位置 $\{(v_{i,1}, PS_{i,1}), (v_{i,2}, PS_{i,2}$ $), ..., (v_{i, n_i}, PS_{i,n_i}) \}$，这里$PS_{i, k}$ 是一组位置相关的数值，来表示这个属性值 $v_{i, k}$ 出现在不同记录中的位置属性。一个简化的知识库如图~\ref{chap:chap03:fig:KB}所示，为了方便理解，这里我们只是使用了一些简化的单词而不是使用原始的属性名。

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{../figures/chap03/KB.pdf}\\
  \smallcaption{一个简化的知识库}
  \label{chap:chap03:fig:KB}
\end{figure}

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{../figures/chap03/labelExample.pdf}\\
  \smallcaption{基于知识库的初始分割和标注例子}
  \label{chap:chap03:fig:labelExample}
\end{figure}


\subsection{基于知识库的初始分割和标注}
这一模块的主要目的是识别出输入文本中所谓的\emph{锚点块}(Anchor Blocks)，就是对一些文本段，我们会根据知识库计算出来一个标注可信度，当这个可信度大于某个阈值时，便可以直接将这个文本段标注成某一个属性。然后，这些识别出来的\emph{锚点块}将被视作一个基础锚点，执行后面步骤中的分割和标注。

\begin{definition}
{\bf (锚点块)}
给定一个输入文本 $I$, 如果它满足下面的两个条件，$I$ 中的一个文本段 $b$ 才可以被标注成具有属性 $A$ 的锚点块：
%
1)没有任何一个其他属性 $A' \in \mathbb{A}$  满足 $p(b, A') >=p(b, A)$ ， 这里 $p(b, A)$ 是我们定义的一个方程用来计算一个文本段 $b$ 属于属性 $A$ 的概率。
%
2) $p(b, A) > \theta$，这个阈值 $\theta$ 是我们自己定义的，用来提高锚点块的标注质量。
\end{definition}

假设一个文本段 $b$ 出现在知识库中的 $m$ 个不同属性$\{(A_1, A_2, ..., A_m\}$ 的属性值里面，这里$A_i \in \mathbb{A}$ ($1\leq i \leq m$)表示 $b$ 的一个可能的属性。对于每一个 $A_i$ ， 假设 $b$ 出现在一组不同的属性值中，并且具有不同的出现频率，我们记做  $\{(v_{i,1}, f_{i,1}), (v_{i,2}, f_{i,2}), ..., $ $(v_{i,r_i}, f_{i, r_i})\}$ ，假设所有属性值都是互相独立的，则我们可以通过下面的公式，计算出这个文本段 $b$ 属于属性 $A_i$ 的概率为：

\begin{equation}
\label{eq1}
p(b, A_i) = \frac{1+\sum_{1\leq j \leq r_i}\sum_{1\leq x \leq f_{i, j}}{\frac{1}{x}}}{1+\sum_{1 \leq k \leq m}\sum_{1\leq j \leq r_k}\sum_{1\leq x \leq f_{k, j}}{\frac{1}{x}}}
\end{equation}

这里，我们在分母使用加1的操作是为了防止分母出现为零的情况。 有了这个公式和我们定义的阈值 $\theta$ ，我们便能够在输入文本 $I$ 中标注出一些\emph{锚点块}了。因为，我们的方法的抽取质量和效率是对阈值 $\theta$ 敏感的，所以我们会通过后面的实验部分找到一个合适的阈值。

注意，一个单独的属性值可能被分割成了两个或更多个文本块，比如图~\ref{chap:chap03:fig:labelExample}中，属性 $D$ 便有两个文本块 $d_1$, $d_3$。考虑到在一个输入文本中每个属性可能有一个属性值，所以，我们可以将这两个\emph{锚点块}和他们之间所有的词语都合并到一起。这里可能出现的一个冲突是，在一个属性的两个\emph{锚点块}之间出现了另一个属性的\emph{锚点块}，通常，一个合适的阈值 $\theta$ 便可以避免出现这种冲突。但是，如果这种情况仍然存在，我们可以通过弃掉冲突\emph{锚点块}中得分更低的那个来解决这个问题。

例子一：给定图 ~\ref{chap:chap03:fig:KB} 中的知识库， 对于一个图~\ref{chap:chap03:fig:labelExample}(1)中输入文本， 我们能够识别出下面这些\emph{锚点块}: $a_2$, $b_1 b_2$, $c_1$, $d_1$, $d_3$ 以及 $f_2$ ，就是图~\ref{chap:chap03:fig:labelExample}(2)中绿色方格这些文本段。另外, 既然 $d_1$ 和 $d_3$ 都是属性 $D$ 的\emph{锚点块}, 我们合并这三个块 $d_1 d_2 d_3$ ，并共同标注为属性 $D$ ，如图~\ref{chap:chap03:fig:labelExample}(3)所示。


\subsection{基于卷积神经网络模型的构建和文本块的标注}
为了处理一些未标注的文本段，比如上述例子中第一步产生的$a_1$， $a_3$ 和 $c_2$，我们需要执行一个基于卷积神经网络模型分类结果的概率标注。简要来讲，我们首先使用知识库训练一个卷积神经网络分类器(CNN分类器)，假设 $|\mathbb{A}|$ 表示给定领域中属性的个数，然后对于每一个输入文本 $s$ ，我们的CNN分类器会生成一个 $|\mathbb{A}|$-维的输出向量 $P(s)=[\rho_1, \rho_2, ..., \rho_{|\mathbb{A}|}]$， 这里每一个 $\rho_{i}$ ($1\leq i \leq |\mathbb{A}|$)表示$s$属于属性 $A_i \in \mathbb{A}$ 的概率。对于每一个未标注的文本段，我们使用我们的CNN模型来决定它属于哪一个属性，然后综合考虑所有未标注文本段的CNN模型输出的概率结果和那些\emph{锚点块}，得到整个输入文本的最佳分割和标注结果。下面，我们解释如何构建CNN模型，并且介绍如何找到最佳的标注方式。

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{../figures/chap03/CNN_des.pdf}\\
  \smallcaption{CNN分类模型的架构图}
  \label{chap:chap03:fig:CNN_des}
\end{figure}

\subsubsection{构建CNN分类模型}
图~\ref{chap:chap03:fig:CNN_des}给出了我们构建的CNN分类模型的架构图，它的输入时一个文本段，输出一个关于所有可能的属性的概率向量。整体来看，我们的CNN模型包括特征输入层，嵌入层，卷基层，最大值池化层，全连接层和最后的Softmax分类器层。特征输入层接受一个输入文本段的特征矩阵，这个特征矩阵包含了丰富的句法和语义特征。嵌入层负责将这些特征值对应到相应的特征向量然后将他们合并在一起。然后我们在特征矩阵上执行卷积操作来提取关于输入文本段的特征，在这一步我们使用了不同大小的过滤核来在输入文本段上尽可能多的覆盖n-gram的文本范围，这样我们就可以生成多层次的，且不同尺寸的特征矩阵。然后，我们再使用最大值池化来对卷积层的结果进行标准化，使其最终都成为相同形状的特征矩阵，且这一步也是进一步抽取特征和降维的操作。池化层得到的特征矩阵再通过一个全连接层，就是一个普通的神经网络，做进一步的特征组合和推理。最终，我们使用Softmax分类器作为我们的输出层，为每一个输入文本段产生分类结果。更多的细节，可以去参考~\cite{sahu2016relation, kim2014convolutional}。

\smallskip
\noindent \underline{\it 特征:}我们为每个输入文本段都构造了一些列丰富的语义特征，包括一些合成的特征。对于文本段中的每一个单词，他的特征矩阵中包括了，提前训练的词向量 Word2vec， 文本段的位置信息，文本段在输入文本中的位置信息，词性标注信息(POS)，文本段长度，以及一种人为构造的归一化类别概率。下面给出每一个特征的详细介绍：

\begin{enumerate}[1)]\setlength{\itemsep}{-1pt}
\item {\bf Word2Vec:} 对于词向量，我们直接使用文章~\cite{th2015evaluating}中用~\cite{mikolov2013distributed}词向量模型在Pubmed文章上提前训练好的词向量，这个词向量模型是由Tomas Mikolov在Google发明，我们在之前的章节有详细介绍。对于中文实验集，我们在自己的中文语料上使用Google的word2vec工具~\footnote{ https://code.google.com/archive/p/word2vec/}训练了一个词向量。

\item {\bf 在文本段中位置特征:} 这个位置特征是指一个单词在输入文本段中的位置，我们之所以使用这个特征，是因为我们观察到，在一些特定的属性值中，一些特殊的词语会出现在相同的位置上。

\item {\bf 在输入文本中的位置特征:} 这个位置特征是指一个单词在原始输入文本中的位置，我们选择这个特征是因为一个单词在文本中的所在位置往往与它属于哪个属性有密切的关联，这个一个比较容易想到的特征项。

\item {\bf 文本段的长度:} 也就是文本段中单词的个数，使用这个特征是因为不同属性的属性值的平均长度往往是有差别的，这说明，文本段的长度是一个非常有区分性的特征。

\item {\bf 语法和词性标注特征(POS):} 构造POS特征的过程，对一句话中的每个单词，依赖这个单词的词性定义和上下文来给它标注一个标签，用以说明这个单词在这句话中的角色。这里，我们使用GENIA tagger~\footnote{ http://www.nactem.ac.uk/GENIA/tagger/}来得到每个词的POS特征。

\item {\bf 归一化类别概率:} 我们同样使用公式~\ref{eq1}来计算一个概率向量$\vec{cp}$，向量每一维的值表示一个单词属于某个属性的概率。

\end{enumerate}

\subsubsection{构建CNN分类模型}
虽然CNN分类器可以直接输出一个输入文本段属于各个属性的概率值，因为已经存在标注了的\emph{锚点块}的位置，所以每个输入文本段只会有几个可能的候选属性，因此，对于CNN分类器的结果，有些概率中是用不着的。例如，在图~\ref{chap:chap03:fig:labelExample}(3)中，文本段 $a_3$ 只可能属于属性 $A$， $B$ 和 $E$
中的一个，因为我们已经有了属性为 $C$，$D$ 和 $F$ 的\emph{锚点块}，且这些\emph{锚点块}都在文本段$a_3$的非邻接位置。我们在图~\ref{chap:chap03:fig:candidate-Labels}列出了其他文本段的所有候选属性列表，其中已经得到的\emph{锚点块}只有一个候选属性。

对一个文本段 $s$ ，另 $\mathbf{A}_c(s)$ 表示一组候选属性，$P(s)=[\rho_1, \rho_2, ..., \rho_{|\mathbb{A}|}]$ 表示$s$ 的CNN模型的输出结果，则文本段$s$ 属于一个候选属性 $A \in \mathbf{A}_c(s)$ 的概率用下面的公式计算得到：
\begin{equation}
\label{eq:2}
p(s, A)=\frac{\rho_{i(A)}}{\sum_{A' \in \mathbf{A}_c(s)}{\rho_{i(A')}}}
\end{equation}
这里 $i(A)$ 表示属性 $A$ 在属性列 $\mathbb{A}$ 中的下标。对于每一个输入文本段，我们都可以使用公式~\ref{eq:2}计算得到它属于每个候选属性的概率值。

例子二：图~\ref{chap:chap03:fig:candidate-Labels}中列出了例子一基于CNN模型得到的各未标注文本段的属于各候选属性的概率值。对于文本段 $a_3$ ，假设它原始的CNN模型输出的概率向量是  $[0.567, $ $0.067, 0.199, 0.133, 0.033]$ ， 因为 $a_3$ 只可能属于属性 $A$，$B$ 或者 $E$ 。因此，我们能计算出文本段 $a_3$ 属于 $A$ 的概率为：$p(a_3, A) = \frac{0.567}{0.567+0.067+0.033} = 0.85$ ，相同的，我们可以计算出属于 $B$ 和 $E$ 的概率 $p(a_3, B)=0.1$ 和 $p(a_3, E)=0.05$。


\begin{figure}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{../figures/chap03/candidate-Labels.pdf}\\
  \smallcaption{例子中文本段的候选属性值和基于CNN模型的变换概率值}
  \label{chap:chap03:fig:candidate-Labels}
\end{figure}


\subsection{输入文本的贪婪式概率标注}
尽管我们的CNN模型能够直接给出输入文本段属于各属性的概率，但是这种标注方式只是单独的考虑了的这一个文本段，没有综合考虑这个文本段与输入文本中其他文本段的关系，因此这种方式可能不能够找到最佳的分割和标注方式。因此，我们在执行每一个概率标注的时候需要整体考虑整个文本的分割和标注状态，事实上，我们期待去找了一种最佳的标注方式来最大化整体的标注概率值得分，而这个得分就是输入文本中所有文本段属于对应属性的概率之和，但是这个过程需要遵循一个条件，这个输入文本中每个属性值只能由位置连续的文本段组成，我们把这个问题定义为\emph{基于CNN的受限分割式信息抽取}(Constrained CNN-IETS Problem)：

\begin{definition}
({\bf Constrained CNN-IETS Problem}) 给定某个领域属性集 $\mathbb{A}$ 以及一个输入文本 $I$ 来做分割式信息抽取任务,，假设 $S(I)=\{s_1, s_2, ..., $ $s_m\}$ 表示从 $I$ 中分割出来的文本段， 其中 $s_1+s_2+...s_m=I$, 并且 $s_i \cap s_j = \emptyset$ ($i \neq j$)，
%
每一个文本段 $s_i \in S$ 都有一组候选属性标签 $\mathbf{A}_c(s)$，则\emph{基于CNN的受限的分割式信息抽取}任务的目的是最大化下面这个方程式：

\begin{equation}
\label{eq:3}
Prob(I, \mathbb{A}) = \sum_{s \in S(I)}{p(s, A_s)}
\end{equation}
, 这里 $A_s \in \mathbf{A}_c(s)$ 是 $s$ 的一个可选属性标签， 所谓\emph{受限}是指，对于任何不相邻的文本段 $s_a \in S(I)$ 和 $s_b \in S(I)$，他们的可选属性标签 $A_{s_a} \neq A_{s_b}$ ，除非他们之间的所有文本段都具有相同的可选属性标签。

\end{definition}

但是，这中基于CNN模型结果的受限的分割式信息抽取问题(Constrained CNN-IETS Problem)是一个NP-hard问题，我们给出下面的定义：

\begin{theorem}
\label{theorem:1}
基于CNN的受限的分割式信息抽取问题是一个NP-hard问题。
\end{theorem}

\begin{proof}
首先，我们假设输入文本中一个没有任何\emph{锚点块}的属性称作\emph{自由属性}(Free-Attribute)，如果在这个输入文本中存在属性值，则它可能出现在这段文本中任何没有标记的位置上。假设，这个输入文本中有 $n$ 个自由属性，我们可以将我们的基于CNN的受限的分割式信息抽取问题约简地看做是旅行商问题(Travelling SalesMan problem，TSP)。另 $V$ 表示一组城市, $A=\{(r,s):r,s\in V\}$ 表示城市之间的路径，$C(r, s)=C(s, r)$ 表示城市 $r$ 和城市 $s$ 之间的路程费用，旅行商问题(TSP)是最基本的路线规划问题，就是找到最小的路程总费用而访问完所有城市。

我们可以将我们的问题简化成为旅行商问题(TSP)：将我们的问题中的 $n$ 个自由属性看做旅行商问题中的那一组城市 $V$ ，将公式~\ref{eq:3}中的 $p(s, A_s)$ 看做是与边 $(A_c, A_s)$ 相关的路费计算方式，当且仅当 $A_c$ 是第 $i$ 次出发的城市。然后，我们的问题就是找到最大的路程总费用而访问完所有城市。因此，我们可以发现，基于CNN的受限的分割式信息抽取问题跟旅行商问题一样，在旅行商问题中需要去全排列列出来所有可能的路线，然后找到最小化花费的旅行路线，我们的是要全排列列出所有可能的文本段组合方式，找到最大概率值得标注方式。于是，理论~\ref{theorem:1}得证。

\end{proof}

但是，列出所有可能的文本段组合方式是一个很不合实际的方式，因为，(1)因为未标注的文本段个数很多，用全排列会产生大量的组合结果，很多组合结果完全没有考虑\emph{锚点块}的位置，是完全没有意义的，这增加了整体标注错误的几率， (2)得到如此多组合结果，处理起来会花费大量的时间，这种方式效率上会大受影响。为了解决这些问题，我们提出了一种\emph{贪婪式概率标注算法}，这个算法基于每个文本段属于所有候选属性的概率值，然后寻找到一个对于当前输入文本的尽可能接近最佳的标注结果。在算法~\ref{alg:1}中详细介绍了这种贪婪式的标注算法：对于一个输入文本 $I$ ，另 $S(I)=\{s_1, s_2, ..., s_m\}$ 表示从文本 $I$ 中分割得到的所有文本段，最开始，一部分这些文本段被定义为\emph{锚点块}，对于剩下的未标注的文本段，我们以一种贪婪的模式来标注他们。首先，我们在输入文本中寻找有没有存在相邻的\emph{锚点块}，比如图~\ref{chap:chap03:fig:labelExample}(2)中的点块 $b_1b_2$ 和点块 $c_1$ ,如果找到了，我们便可以将输入文本分割成两部分，比如像图~\ref{chap:chap03:fig:labelExample}(3)中的这条分割线。然后，对于每一个分区，基于CNN模型的输出概率向量，使用公式~\ref{eq:2}，我们便可以计算出来每个文本块属于它的所有候选属性的概率值。在这些所有未标注的文本块中，每一次我们只贪婪地选择一个出来进行标注，记做$s_c$ ，这个选出来的文本段的概率应该是剩下这一分区中未标注文本段中概率值最高的那一个。然后，我们根据概率值为文本段 $s_c$ 标注上对应属性，记做 $A_c$，然后，我们立即检测是不是存在一个非邻接的文本段 $s'$ ，且这个文本段 $s'$ 也已经具有属性 $A_c$ ，如果出现这种情形，我们需要检查 $s'$ 和 $s_c$ 之间是不是已经存在标注了其他属性的文本段，如果，这种情况出现了，我们便取消这次对 $s_c$ 的标注，然后重新选择具有第二高概率值的文本段来进行相同的标注。如果这种情形没有发生，我们需要将 $s'$ 和 $s_c$ 之间所有的文本段合并形成一个新的文本段，且标注为 $A_c$ 。我们重复执行这个过程，直到输入文本 $I$ 中没有可以被标注的文本段为止。注意，在这个过程中，我们定义了一个阈值 $\lambda$ 来防止产生低质量的标注。



\subsection{基于序列和位置特征的结果修正}

\section{实验结果与分析}

\section{本章小结}




