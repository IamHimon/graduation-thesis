

\chapter{系统的实现与展示}

\section{Tensorflow介绍}
TensorFlow 是一个使用数据流图进行数值计算的开源软件库。图中的节点代表数学运算， 而图中的边则代表在这些节点之间传递的多维数组（张量）。这种灵活的架构可让您使用一个 API 将计算工作部署到桌面设备、服务器或者移动设备中的一个或多个 CPU 或 GPU。 TensorFlow 最初是由 Google 机器智能研究部门的 Google Brain 团队中的研究人员和工程师开发的，用于进行机器学习和深度神经网络研究， 但它是一个非常基础的系统，因此也可以应用于众多其他领域。

\section{CNN模型构建细节}
本文提出的基于深度卷积神经网络(CNN)的信息抽取模型，除了事先构建知识库和第一步做初始的文本分割，其他部分的执行都是基于我们构建的CNN分类器模型的输出结果。因此，我们的分类器模型需要能更准确的刻画各属性值的区别，有更高的准确率，但是，我们知道，卷积神经网络(CNN)的优势在于它能充分抽取特征和自由组合特征的能力，但是CNN模型的这个优势，从另一个角度来讲，恰恰也是它的一个劣势――缺乏模型解释性。因此，我们在使用卷积神经网络来构建深度学习模型时，更多时候我们只能把它看做黑盒子。但是，我们还是可以通过加入一些模型训练技巧，探索合适的超参数，改变模型结构来提高模型的表现力。在这一节，将详细介绍我们构建CNN分类模型的一些细节(我们将不再给出实验结果，我们会结合实验结果尝试解释如此选择的原因，从而得到一些经验相关的知识)：
\vspace{-2pt}
\begin{enumerate}[1)]\setlength{\itemsep}{-1pt}

  \item  \textit{CNN模型结构:} 我们使用了文章~\cite{kim2014convolutional}中类似的模型结构，在输入层之后，是一个多卷积核的卷积层，然后接一个池化层，最后是一个softmax分类器。卷积层和池化层的层层叠加构成了卷积神经网络模型(有些模型没有池化层)，通常，现实中常用的基于卷积神经网络的深度学习模型都会有很多层的卷积层加池化层，特别是在做图像和语音的任务中。因此，我们在构建自己的模型时，尝试增加卷积层和池化层的数量，但是通过实验我们发现，这并没有提升模型的表现力，反而有略微的相反作用，并且更重要的一点是，增加了层数模型训练时间会有明显的增加。模型层数多就代表模型需要学习的参数更多，当我们要做图像识别时，模型的输入是图像的像素矩阵，这个矩阵的特征密度非常高，因此我们需要一层一层的，从低阶到高阶的，从部分到整体的对输入矩阵进行处理，并在每一层进行特征的抽象和组合。图像中特征的刻画是非常复杂的，比如从低阶的边角特征组合成不同动物是需要在非常高阶上组合的，因此，增加模型的层数，才能够用更多的参数来刻画这种高阶的特征。在我们的应用场景中，首先因为我们的模型输入矩阵是训练好的word2vec，虽然是一种压缩的高密度文本表征方式，但是这种‘高密度’也是相对于one-hot编码而言，相比较图像的像素矩阵，所包含的特征密度应该是差的很远，因此我们不需要这么深的网络。另外，我们训练CNN模型的输入是来自于知识库中的属性值，它的文本长度相对较短，即使最终将所有输入都归一化之后，也不会是很大的输入矩阵，这种输入矩阵的大小也不需要我们使用过深的网络。网络越复杂过拟合的可能性越大。因此，这种单层卷积加单层池化的结构是最适合我们的应用场景的。


  \item \textit{CNN模型通道:} CNN网络中的不同\emph{通道}可以解释为‘看’输入矩阵的不同角度，比如，在做图像识别时，我们有红，绿，蓝三个通道，模型在三个通道上同时运行，然后以某种方式组合起来。在我们的模型中，也采用了类似的模式。我们使用静态通道和动态通道，静态通道是指，对于我们的输入矩阵――word2vec，在训练我们的模型时此输入矩阵保持不变，而模型的其他参数随着模型训练而改变。动态通道是指，不仅模型的其他参数随着训练的过程而改变，输入的word2vec也会在训练过程中进行微调。具体来讲，在训练时，同时给模型两组训练数据，输入的都是词向量，然后在每一组训练数据上，每一组词向量就看做是一个通道，然后每个卷积核会同时在两组训练数据上执行，但是，梯度的反向传播只在其中一个通道上进行。这样就可以保持一个通道上输入词向量不变，也就是静态通道，而微调另一个通道的输入词向量，也就是动态通道。而静态通道和动态通道的实现在Tensorflow中也是比较简单，只需要在 $Variable$ 类中设置$trainable=True/False$，然后相应的改变后面步骤中张量的维度就行了。这种策略在文章~\cite{kim2014convolutional, zhang2015sensitivity} 中有详细的讨论。


  \item \textit{卷积核数量和大小:} 我们的输入是一句话，假设规则化之后，输入文本长度是 $d$ ，词向量的维度为 $V$，则输入矩阵便是一个 $d \times V$ 的矩阵，每行代表一个词语。类似于语言模型中的 $n-gram$ 思路，我们需要将相邻的若干个词语同时来考虑，也就是需要卷积核每次读入 $m$ 行词向量，这个行数就看做是卷积核的大小。当确定了卷积核的大小之后，不同参数的卷积核可以抽取出不同的特征，因此看做是不同的卷积核。我们通过随机设定卷积的参数，便可以得到若干个不同的卷积核，每个卷积核都能够抽取出一个特征图(future map)，最终再综合考虑所有的特征图。为了充分的抽取特征，在卷积层中，我们使用了不同大小的卷积核，并且每种大小的卷积核中设置一定数量的卷积核。但是文章~\cite{zhang2015sensitivity} 中证明了，对于不同的数据集，卷积核数量和大小的选择是不同的，并且，对于每个通道使用相同大小的卷积核往往具有更好的表现。因此，我们通过实验得出了对于我们的每个数据集来说，最合适的选择，如表~\ref{table:6} 所示。其中卷积核大小的选择空间设定为：$[3,5,7,9,10,15,20,25]$，数量的选择空间设定为 $[50,100,200,300,400,500,600]$。


  \item \textit{卷积方式:} 卷积方式有两种，宽卷积(Wide convolution)和窄卷积(Narrow convolution)，这两种方式的主要区别就是对于输入矩阵的边界值得处理。我们的模型中使用宽卷积，就是使用零填充(zero-padding)，这样就可以完全覆盖输入矩阵中所有的值，得到更大或者相同大小的输出特征图。


  \item \textit{激活函数:} 我们在不同的数据集上比较了几种激活函数方式，$Relu$， $tanh$，特殊的，我们也测试了不使用激活函数($NoA$)时的表现，因为文章~\cite{zhang2015sensitivity} 中发现，在有些数据集中，不使用激活函数却得到了更好的结果。经过我们的试验发现，在引文数据集中，不使用激活函数取得了更好的实验结果。


  \item \textit{池化方式:} 在我们的模型中，池化层放在卷积层后面，池化的作用有两个，第一个作用是降低维度，第二个作用是统一输出形式。在我们的模型中，使用 $1-max \; Pooling$，也是比较通用和有效的一种 $Pooling$ 方式，并且 $Pooling$ 窗口的大小就是输入文本的长度，这样每个经过卷积核最终只输出一个值。


  \item \textit{正则化:} 我们使用两种正则化策略，一个是 $dropout$ ，一个 $l2$ 正则。我们使用了文章~\cite{zhang2015sensitivity}中相同的办法来寻找最佳的值，通过实验，我们得到图~\ref{table:6}中的结果。其实，正则化对于模型的影响非常小，我们比较不同正则化值得区别也非常小。这可能是因为，我们的模型只是一个交浅层的网络，模型复杂度不高，所以正则化的作用便没有那么明显。同时，我们也尝试在我们的模型中加入 $batch \; normalization$ 的技巧 \cite{ioffe2015batch}，发现并不会对模型的表现起到明显的帮助，只是稍微缩减了一点模型的训练时间。在 $Tensorflow$ 中，我们可以直接调用高度抽象的函数 $batch \; normalization$ 来实现。在试验中，$dropout$ 值的选择空间设定为： $[0.1,0.2,0.3,0.4,0.5]$， $l2$ 中的正则阈值选择空间设定为： $[3, 5,10,20,30,40, 50]$ 。
      
  \item \textit{$batch \;size$:} batch size的选择是一个需要权衡的考虑，没有固定的选择策略。在我们的模型中，选择设定 $batch \;size$ 为64。
\end{enumerate}

对于上面提到的参数，我们使用文章~\cite{zhang2015sensitivity}中相同的方法，通过较粗粒度的网格搜索($grid \; search$)来给模型选择一套合适的参数。具体的过如表格~\ref{table:6}所示。

\begin{table}%[!hbp]
\centering
\topcaption{ CNN模型在不同数据集上的超参选择}
\label{table:6}
\begin{tabular}{|l|c|c|c|}
\hline
\diagbox{参数项}{参数值}{数据集} & 引文数据集 & 二手车 & 二手房 \\
\hline
卷积方式 & 宽卷积 & 宽卷积 & 宽卷积 \\ \hline
池化方式 & ${1-max \; Pooling}$ & ${1-max \; Pooling}$ & ${1-max \; Pooling}$ \\ \hline
激活函数 & $NoA$ & $Relu$ & $Relu$ \\ \hline
卷积核大小和数量 & ${(9,9)/200}$ & ${(10,10)/100}$ & ${(5,5)/400}$ \\ \hline
通道数 & 静态+动态 & 静态+动态 & 静态+动态 \\ \hline
正则化 & ${dropout:0.5, \; l2:20}$  & ${dropout:0.4,  \;l2:40}$ & ${dropout:0.4, \; l2:20}$ \\ \hline
\end{tabular}
\end{table}

\section{系统架构及展示}
为了方便执行抽取任务，方便移植到其他数据集，且可视化我们抽取模型的每一步的结果，我们做了一个自动化训练模型和抽取任务可视化的系统。整体来讲，这个系统主要分为下面三个部分：

\vspace{-2pt}
\begin{enumerate}[1)]\setlength{\itemsep}{-1pt}
  \item \textit{模型训练调参与训练集初始化}

    \begin{itemize}
    \item[*] 模型结构设计和参数设定：可以在网页上直接手动设计模型结构，制定超参的数值。
    \item[*] 构造知识库（$Knowledge \; Base$），加载、处理和预览训练集，选择性构造特征：
手动选择数据预处理相关参数，选择要构造的特征，并预览训练数据的初始化结果等。
    \item[*] 深度学习模型训练过程可视化展现（TensorBoard）。
    \end{itemize}

  \item \textit{输入文本执行抽取任务}

   \begin{itemize}
    \item[*] 输入一个文件，包含多条文本记录：
对这个文件抽取结果的展示，即展示每条记录的属性值和相应属性名，BPSM的展示。
    \item[*] 输入一条文本记录：
展示模型每一步的中间结，（a）KB-based Blocking，展示每一块，标注出Anchor-block和Unknown-block。（b）CNN-labeling，展示归一化的CNN输出概率，展示目前分块和标注结果。（c）BPSM-amendment，展示BPSM模型以及最终的抽取结果。
    \end{itemize}

  \item \textit{将结果存入到结构化文件中输出}

\end{enumerate}


下面详细介绍每个模块：

\begin{figure}
  \centering
  \subfigure[设定模型参数]{
    %\label{fig:subfig:a} %% label for first subfigure
    \includegraphics[width=2.5in]{../figures/chap04/set_model_para.png}
  }
  \subfigure[加载知识库]{
    %\label{fig:subfig:a} %% label for first subfigure
    \includegraphics[width=2.5in]{../figures/chap04/load_kb.png}
  }
  \subfigure[训练模型]{
    %\label{fig:subfig:a} %% label for first subfigure
    \includegraphics[width=2.5in]{../figures/chap04/train_model.png}
  }
  \caption{模型初始化与训练}
  %\label{fig:subfig} %% label for entire figure
\end{figure}

\begin{figure}
  \centering
  \subfigure[segment]{
    %\label{fig:subfig:a} %% label for first subfigure
    \includegraphics[width=2.5in]{../figures/chap04/segment-one.png}
  }
  \subfigure[cnn-label]{
    %\label{fig:subfig:a} %% label for first subfigure
    \includegraphics[width=2.5in]{../figures/chap04/cnn-label-one.png}
  }

  \subfigure[amendment]{
    %\label{fig:subfig:a} %% label for first subfigure
    \includegraphics[width=2.5in]{../figures/chap04/amendment-one.png}
  }
  \subfigure[BPSM]{
    %\label{fig:subfig:b} %% label for second subfigure
    \includegraphics[width=2.5in]{../figures/chap04/BPSM-one.png}
  }
  \caption{输入一条记录}
  %\label{fig:subfig} %% label for entire figure
\end{figure}


\begin{figure}
  \centering
  \subfigure[segment]{
    %\label{fig:subfig:a} %% label for first subfigure
    \includegraphics[width=2.5in]{../figures/chap04/segment-mul.png}
  }
  \subfigure[cnn-label]{
    %\label{fig:subfig:a} %% label for first subfigure
    \includegraphics[width=2.5in]{../figures/chap04/cnn-label-mul.png}
  }

  \subfigure[amendment]{
    %\label{fig:subfig:a} %% label for first subfigure
    \includegraphics[width=2.5in]{../figures/chap04/amendment-mul.png}
  }
  \subfigure[BPSM]{
    %\label{fig:subfig:b} %% label for second subfigure
    \includegraphics[width=2.5in]{../figures/chap04/BPSM-mul.png}
  }
  \caption{输入一个文件}
  %\label{fig:subfig} %% label for entire figure
\end{figure}




\section{本章小结}


