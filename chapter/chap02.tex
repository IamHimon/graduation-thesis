\chapter{相关理论及方法}
\label{chap:chap02}
本章首先介绍了深度学习用于自然语言处理的一般处理方案，介绍了深度学习在一些信息抽取领域的应用，并分析了深度学习用于自然语言理解上的优缺点。然后介绍了信息抽取的相关概念及其定义，重点阐述了分割式信息抽取的问题定义。最后详细介绍了常见的分割式信息抽取方法，并分析了各类方法的优缺点。

\section{深度学习在自然语言处理上的应用}
最近几年来，深度学习架构和算法在诸如图像识别和语音处理上取得了突破性的进展，不断刷新各个榜单，应用场景越来越多，在工业界也有越来越多的落地产品，可以说深度学习的发展引导了这一波人工智能的热潮。图像和语音是自然界的产物，它的特征是一种更自然更丰富的表征，而深度神经网络的优势就在于它的特征抽取和特征组合能力，这是深度学习能在图像和语音上取得成功的原因。然后，人类的语言，属于人类文明创造的事物，不具备自然的表征能力，因此表达形式更主观，具有高度结构化、高抽象化、数据量相对小等特点，是一种更粗粒度的表现形式。因此，一开始，深度学习在自然语言处理领域的应用效果非常一般，随着算法的发展和思路的提升，特别是词向量的引入使得我们可以更好地将自然语言向量化，现在已经在越来越多的自然语言处理任务中证明，深度学习可以有更优秀的表现，甚至是最佳的表现。比如，深度神经网络模型在诸如文本分类，关系抽取，命名体识别，机器翻译等任务中的表现已经大大超越了传统方法，并且深度学习也在继续拓展它在自然语言理解上的应用。

在很长一段时间，自然语言处理的研究方法都是采用这些浅层的模型，来学习到非常高纬且稀疏的特征表示。在传统的机器学习中，我们使用各种算法的基础是需要手工设计特征，因此特征工程是一个非常基础性的工作。只有当人们对特定领域的知识有非常透彻的理解时，才能构造出足够多、足够优质的特征。比如，对于命名题识别任务，当要识别地名和机构名，我们首先需要构造出如下的特征列表：
\begin{figure}[!h]
  \centering
  \includegraphics[width=0.45\textwidth]{../figures/chap02/ner-features.jpg}\\
  \smallcaption{命名题识别任务特征列表}
  \label{chap:chap02:fig:NER_example}
\end{figure}
然后将特征喂给某个机器学习算法，比如线性分类器，分类器构造出目标函数，再通过凸优化策略不断调整模型的权重和偏置，使误差优化到最小，在这样的过程中为这些特征找到最合适的权重。这样的人工设计的特征，常常需要定义过多，并且一般是不可能做到完整的，需要花费大量的时间去设计和验证。

然而在自然语言处理上，由于语言的特殊性，设计有价值的特征是非常困难的事情，想要从如此抽象的文本信息中抽取出有用的特征，必须经过不断的迭代和实验，这是非常耗时的。深度学习是一种端到端的模型，我们只需要提供输入，不需要做额外的特征工程，深度神经网络会自动的进行特征抽取、特征组合。我们知道，图像信息是以像素点作为表示单位，声音是以声波作为表示单位，这些都输入较为底层的原始输入信号，可以直接数字化，作为深度神经网络的输入，进行端到端的模型训练。然而，一句文本却是以一个独立的单词或字组成的，虽然单词或字是独立的，但是组合成一起就成为了一句有意义的表述，如何将一句文本向量化表述，同时体现单词之间和字之间的联系，是一个重要的切入点。近些年，基于稠密向量表示的深度神经网络在很多自然语言处理任务上取得了更好的表现，也就是词向量的横空出世，可以说是将深度学习引入到自然语言处理领域的重要转折点。下面介绍一下词向量以及词向量结合深度学习在自然语言中的应用。

\subsection{词向量}
想要将自然语言理解问题转化成机器学习问题，首先需要将自然语言数字化，就如何将图像、语音数字化相同。最直观的一种方式，也是最传统的方法――采用 one-hot 的编码，将每个单词表示成  $R^{|V| \times 1}$ 纬的向量，其中  $|V|$ 表示词典中词语的个数，对于第  $i$ 个词语的向量，只有  $i$ 下标处为1，其它纬都为0。比如'food', 'eat', 'laptop' 作为前三个词，则他们的 one-hot 向量表示为：
$$w^{food} = [1,0,0,0,\cdots,0] \qquad w^{eat} = [0,1,0,0,\cdots,0] \qquad w^{laptop} = [0,0,1,0,\cdots,0] \qquad \cdots$$

这种向量的表示形式，实现了将自然语言数字化的目的，在实际的应用中，配以Hash的处理，再结合一些机器学习算法就可以很好的解决自然语言处理领域各种主要的任务了。但是它存在两个最主要的问题：

(1)纬度灾难[Bengio 2003]。当某一语料词典数目过大，这个向量的纬度会变得很大，数据会变得特别稀疏，导致统计语言模型会出现很多为零的条件概率，这需要花费大量的精力来处理零概率的问题。这导致在训练模型进行矩阵时计算会非常困难，特别是应用在深度学习模型上，容易导致纬度灾难。

(2)不能很好地刻画单词之间的相似性。也就是常常说的'词汇鸿沟',对于两个词性或者语义上有关联的词语，他们的向量表示应该能体现出一些关联性和区分性，但是使用 ont-hot 编码无法实现这个目的。比如对于 'food','eat','laptop'这三个词：
$$(w^{food})^{T} \times w^{eat} \quad = \quad (w^{eat})^{T} \times w^{laptop} \quad = \quad 0$$
他们的向量相乘结果是相同的，且都等于0，这就无法体现'food'和'eat'之间的相关性，也不能体现这三者的区分性了。

因此我们想要去学习出一种使用低纬度空间的分散式表示来向量化一个词语，同时又需要将相似性和关联性融入到这个低维向量中，这就需要我们用语言模型来训练这个词向量，因此构建词向量的原理依据就是具有相似含义的词语往往出现在相似的语境之中。使用神经网络训练语言模型在[Bengio 2003]中被首次提出，作者阐述了传统基于统计的语言模型的主要问题，并说明了使用神经网络训练语言模型的优势。词向量得到革命性的发展是[Mikolov et al]提出的CBOW模型和Skim-gram模型，也就是著名的\textbf{word2vec}。

首先对词典中的所有单词进行one-hot编码，词典中词语数目为 $V$。CBOW 模型是使用周围的 $2C$ 个词语来预测目标词语，比如对于一句话"狗 喜欢 吃 熟的 骨头 "，我们可以将$ \{'狗'， '喜欢', '熟的', '骨头'\} $ 看做周围语境(两边各 $C$ 个词语)，来预测和生成目标词语$\{ '吃'\}$，而Skim-gram 模型是跟这个思路相反的，它使用目标词语来预测它周围的 $2C$ 个词语。CBOW 模型的简单形式如图~\ref{chap:chap02:fig:CBOW} 所示,这里只考虑了句子中的一个词语。CBOW 模型本质上就是一个三层的全连接神经网络(包含一个隐藏层)，输入层和输出层都具有 $V$ 个神经元结点, 隐藏层具有 $N$ 个神经元结点，往往 $N$ 要大大小于$V$。则CBOW模型的输入层就是通过one-hot 编码的 $V$ 维度向量，这样one-hot 编码的向量的每一维的值便可与输入层每个神经元结点对应起来。
更一般的形式如图~\ref{chap:chap02:fig:CBOW_general} 所示，模型的输入就是目标词语的周围语境，即 $C$ 个词语。假设输入的 one-hot 词向量表示为 $x^{(i)}$，模型的输出表示为 $y^{i}$，这个模型的唯一输出 $y^{i}$ 也就是我们的目标词语 $y$, 同样是使用one-hot编码的向量。另外，我们需要创造两个矩阵，也是这么模型输入层到隐层和隐层到输出层的权重矩阵， $W^{1} \in R^{N \times |V|}$ 和  $W^{2} \in R^{|V| \times N}$ , 这里 $N$ 就是隐层神经元数目，也表示最终词向量的纬度空间。

这个语言模型的训练过程可以分解为如下几个步骤， 对于第$i$个目标词语：

(1)首先生成 $2C$ 大小上下文词语的one-hot向量，记做 $( x^{i-C}, \cdots , x^{i-1}, x^{i+1}, \cdots , x^{i + C})$

(2)对于这个上下文，计算其中所有词语的嵌入向量 $( u^{i-C} = W^{1}x^{i-C},  u^{i-C+1} = W^{1}x^{i-C+1}, \cdots ,u^{i+C} = W^{1}x^{i+C} )$

(3)计算这些向量的平均向量 $h = \frac{u^{i-C}+u^{i-C+1}+\cdots+u^{i+C}}{2C}$

(4)计算输出层的得分向量 $z = W^{2}h$

(5)使用softmax将输出层得分向量变成归一化的概率值 $ \widehat{y} = softmax(z)$

(6)使用这个输出概率值与真正的目标词语 $y$ 比较，通过计算交叉熵 $H(\widehat{y^{i}}, y^{i})$ 来定义损失函数，再通过梯度下降一步一步调整权重矩阵，以此来优化这个模型。

而最终训练处的模型的输入层到隐藏层的权重矩阵 $W^{1}$ 才是我们需要的词向量表示，具体来讲，第 $i$ 个词语的词向量就是 $W^{1}$ 的第 $i$ 列的向量 $w^{i} \in R ^{N \times 1}$ 。图~\ref{chap:chap02:fig:word2vec_sample}展示了词向量样例，可以看到可以体现词语之间的相似性。当然这只是最基础的词向量构造方法，还有很多版本的词向量构造思路，最近基于字符级别的字符向量可以捕获到更多句法级别的信息，展现了巨大的应用潜力。

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.45\textwidth]{../figures/chap02/word2vec_sample.png}\\
  \smallcaption{词向量的样例}
  \label{chap:chap02:fig:word2vec_sample}
\end{figure}


\begin{figure}[!h]
  \centering
  \includegraphics[width=0.45\textwidth]{../figures/chap02/CBOW.png}\\
  \smallcaption{CBOW模型的简单形式}
  \label{chap:chap02:fig:CBOW}
\end{figure}


\begin{figure}[!h]
  \centering
  \includegraphics[width=0.45\textwidth]{../figures/chap02/CBOW_general.png}\\
  \smallcaption{CBOW模型的一般形式}
  \label{chap:chap02:fig:CBOW_general}
\end{figure}

理解了词向量的构造原理，就解决了深度学习应用在自然语言处理上最基本的问题――输入的表示。剩下的就是根据具体问题，如何选择模型，如何设计解决方案的问题了。有很多深度学习的模型可供选择，比如深度神经网络DNN, 卷积神经网络CNN，循环神经网络RNN，长短期记忆神经网络LSTM，GRU(LSTM的一种变体)，递归神经网络，基于注意力机制的深度神经网络模型，Encoder-Decoder模型，记忆网络模型等等。

基于深度学习模型的方法已经在很多自然语言理解领域取得了更好的表现，比如,
在文本分类问题上，文章[1][2]提出了基于卷积神经网络的模型，文章[1]使用比较普通的卷积神经网络结构，文章[2]中使用一种多层的交织最大池化层和卷积层的网络结构。文章[3][4] 都使用基于循环神经网络模型，两者的对比体现了LSTM 相比较普通RNN 的优势，并且文章[4] 使用的是一种树形长短期记忆网络(Tree-LSTM)，证明了树形LSTM 相比于双向的LSTM 的优势，这表明Tree-LSTM 能够更好的捕捉到自然语言的句法信息。文章[5] 使用的基于记忆网络模型目前为止取得了最优的表现。

在机器翻译任务上，文章[]使用一个四层的 seq2seq 模型，使用LSTM， 然后用这个模型对从传统SMT方法产生的1000个候选翻译重新排序。文章[]使用一个带有 8 个编码器和 8 个解码器的深度 LSTM 网络组成，并使用了注意机制（attention）和残差连接（residual connections）。最近，[]提出了一种基于CNN的seq2seq模型，输入的每个词语通过CNN由并行方式组成的注意力架构计算出其表示，另外，解码状态需要同时考虑之前已经产生的结果。[]中不在是使用RNN或者CNN来作为encoder-decoder的模型基础，只使用注意力机制处理序列模型相关的问题，比如机器翻译，这样可以高度并行的工作，在提升翻译性能的同时也大大提升了翻译速度。

在信息抽取任务上，文章[]提出的基于卷积神经网络的模型，特殊之处在于作者将位置信息与词向量组合作为模型的输入矩阵，在关系抽取上取得了非常好的表现。文章[]利用共享神经网络底层表示来进行联合学习，同时进行实体识别和实体的关系抽取。具体来讲，对于输入的句子共享词向量输入层，通过双向LSTM对输入进行编码，然后分别使用LSTM进行命名体识别和CNN来进行关系分类，这种方法可以很好的解决传统的流水线方法的很多弊端。

以上列出的一些自然语言处理的几个领域，使用深度学习已经取得了突破性的进展，在其他一些领域，比如POS标注，语义解析，问答系统等，使用深度学习模型也都是目前最好的途径。当然也有很多工作是使用深度学习结合传统的机器学习算法实现的，深度神经网络具有它天生的优势，会有更多的应用场景。



\section{分割式信息抽取概念以及现有的方法}




实体匹配是指找出同一数据源内或不同数据源间指代同一实体的数据库记录\ucite{Fan2009Reasoning,Arasu2009Record}，这些实体具有不同的表达形式。以手机实体为例，实体匹配就是指找出表~\ref{tmallTable}和表~\ref{pconlineTable}中表示同一款手机的数据库记录。通过观察，发现实体对$<r_1, s_1>$，$<r_2, s_2>$，$<r_3, s_3>$，$<r_4, s_4>$，$<r_5, s_5>$ 分别表示同一实体，尽管它们的某些属性在表达形式上不同，但在现实世界中它们是同一实体，而实体对 $<r_6, s_4>$ 虽然具有相同的产品名称“4s”，但是本质上它们是不同的实体。实体匹配近年来一直被广泛地研究，其应用领域涉及医疗卫生\ucite{Clark2004Practical}、信息检索\ucite{Hajishirzi2010Adaptive}、商业数据管理\ucite{Su2010Record} 等。

实体匹配方法有多种分类方式，本文主要从数据的角度进行分类，大致可分为三类：基于内容的匹配方法、基于结构的匹配方法和基于混合模式的匹配方法，图~\ref{chap:chap02:fig:recordApproaches} 给出了实体匹配方法分类树\ucite{dorneles2011approximate}，下面对已有的实体匹配方法做简单的介绍。
\begin{figure}[!h]
  \centering
  \includegraphics[width=1\textwidth]{../figures/chap02/record.pdf}\\
  \smallcaption{实体匹配方法分类}
  \label{chap:chap02:fig:recordApproaches}
\end{figure}

\subsection{基于内容的匹配方法}

基于内容的匹配方法（Content-based Matching Methods）主要通过一个相似度函数（如edit distance similarity，Q-gram similarity 等）度量数据值上的相似性，进而比较两个实体是否匹配。这些数据值可以是数据表中的属性值，XML 文档中标签值等。此类方法大致可分为两大类：基于Atomic values 的方法和基于Aggregated values 的方法。

（1）基于Atomic values 的方法通过相似度函数比较原子值的相似性得分，又可细分为基于字符的方法\ucite{Cohen2003A}（character-based）和基于符号的方法\ucite{Dorneles2004Measuring}（token-based），前者进行字符间的比较，比如Levenshtein 方法，Jaro 方法和Q-grams 方法，后者则是进行符号之间的比较，这些符号可以是单词，或者子串，比如SoftTFIDF 方法，MongeElkan 方法和Jaccard 方法。此外，还有一些扩展方法利用机器学习技术构建相似度函数进行实体匹配的\ucite{bilenko2003adaptives}。

（2）基于Aggregated values 的方法考虑如何对数据集中单个属性的相似度进行组合而匹配的，可细分为基于代数的方法\ucite{Motro1988VAGUE} （Algebraic formula-based）和基于AI 技术的方法\ucite{Schallehn2004Efficient}。前者使用一些度量方式结合属性集合中单个属性值的相似性进行匹配，进而得到实体的相似度，如Euclidean distance，Vectorial model 等，后者则是利用一些AI 技术和关联规则、聚类方法等进行匹配的。此外，还有一些基于有序邻居的方法（Sorted Neighborhood Method，简称SNM）\ucite{hernandez1998real}，该方法利用主属性值发现比较靠近的实体。如表~\ref{tmallTable}中的“G9098”和表~\ref{pconlineTable} 中的“G9098”可通过主属性判断是否为同一实体。但是这类方法容易受到表达方式多样的影响，导致匹配的准确性不高。

基于内容的匹配方法又可根据数据集中属性的类型进行分类，可细分为基于主属性数据的匹配方法和基于非主属性数据的实体匹配方法，其中基于非主属性数据的实体匹配方法又可细分为基于结构化非主属性数据的匹配方法和基于文本类型非主属性数据的匹配方法。

（1）基于主属性数据的匹配方法大都是通过各种字符串相似性度量方法进行实体匹配,如基于字符的算法（如Edit distance\ucite{Cohen2003A}，Q-gram\ucite{Aizawa2005A})和基于符号的算法（如atomic string\ucite{Monge1996The}，WHIRL\ucite{Cohen1998Integration}），也有一些混合的方法被提出\ucite{Xiao2008Efficient}，这些方法或是直接用于实体匹配或是间接辅助进行实体匹配，有的方法着重于匹配效率的提升，如Q-gram算法，而有的方法着重于提高匹配的质量，如WHIRL。文献\cite{bilenko2003Adaptive}提出了两种可学习的文本字符串相似度度量方法，包括可学习的字符串编辑距离算法和使用SVM的向量空间度量算法，针对数据表的不同的字段使用不同的方法，在实体匹配过程中该方法具有较好的匹配准确性。近几十年来，一些基于分类\ucite{Jiang2013Entity}和语义网络\ucite{Dhamankar2004iMAP}的方法也有被提出，但是这类方法存在着灵活性差，时间开销大等问题。基于主属性的匹配方法容易受到数据表达方式多样的影响，含义相同的主属性值形式上可能千差万别，而含义不同的主属性值也可能表示同一实体。因此，没有任何一种相似度度量方法可以非常准确地度量所有主属性值之间的相似度。此外，这些实体匹配方法的准确率和召回率对相似度阈值也很敏感，阈值的设定会极大地影响匹配的结果。

（2）鉴于主属性值在表达方式多样化和匹配准确性低等方面的劣势以及结构化非主属性在数目上的优势，一些基于结构化非主属性数据的实体匹配算法被提出，通常是根据结构化数据的相似性\ucite{Koudas2006Record}或关联性\ucite{Parkhomenko2009Sparse}进行实体匹配，常见的结构化数据如数值型数据、日期数据和短字符串数据等。文献\cite{Chaudhuri2007Example}提出了一种使用正反例的方式并结合相似度连接（similarity join）和相似度结合（similarity unions）以及用户指定的相似度函数构建SJU 操作树,通过该结构查询某种适用于待匹配实体属性的相似性度量算法，使得每个属性以最合适的相似性算法进行实体匹配，该方法不仅在小中型数据集上表现不错，而且在大型数据库上也适用。此外，也有一些方法\ucite{Jin2003Efficient}将结构化非主属性值映射到多维欧几里得空间中，结合给定的合并规则利用属性的相似性选择一个最优的属性集合，应用多维相似性连接判断实体对的相似度。文献\cite{Wang2012Can}中提出了一种基于结构的方法进行实体匹配，该方法使用结构化非主属性创建类似于决策树的Matching tree 进行实体匹配，该Matching tree 是一种基于0-1结构的二分树，通过一定的概率计算方式选择属性作为Matching tree 的节点，然后从根节点到叶节点遍历进行匹配，但是这种方法存在适用性问题，当属性值出现缺失时，此次的匹配过程便无法正常进行。

（3）虽然基于结构化非主属性数据的实体匹配算法可以发现匹配的实体，但是当没有足够的可用的结构化数据足以反映实体匹配的结果时，也就是说即使实体具有相同或相似的结构化数据，仍有可能不是匹配的实体，在该情形下基于结构化数据的实体匹配算法很难达到较好的匹配效果，甚至产生更多的误匹配。作为结构化数据的补充，非结构化的文本数据经常出现在各种类型的数据集中。由于每个文本数据中包含了十几个句子甚至更多，而且伴有一些噪数据的出现，使得传统的字符串相似度算法并不能直接的应用。使用文本数据进行实体匹配的关键一步是如何从包含噪数据的文本数据中获取重要信息。近来有一些使用文本数据进行实体匹配的方法被提出。文献\cite{ektefa2011threshold}提出了一种混合算法，该算法同时计算文本之间的字符串相似性得分和语义相似性得分，但是字符串相似性只是简单应用了Jaccard 算法，而语义相似度算法也是简单定义了一些字段（如“Address”，“City”，“Phone”，“Type”等），且是在WordNet的辅助下获取相应信息的，因此该方法只使用一些特定的数据集，当换成其他数据集时效果会大大降低。文献\cite{gao2011web}提出了一种基于语义特征的方法，该方法定义了一个形如\emph\{time，location， agentive，objective，activity\} 语义特征向量，然后训练一个分类器可以根据实体的特征向量用以发现重复的实体，但是这种方法也受限于特征向量的维度，因为有些文本不包含这些特征向量中的信息。因此，此方法很难应用于其它数据集。近来，有一些利用主题模型的方法进行实体匹配，但有些情况下自由文本是对实体某一主题的多个方面进行描述，这些方面共享相同的主题，并没有明显的不同，因此直接使用这些主题模型很难获得较好的匹配效果。此外，文献\cite{Kim2012Method}根据单词的依赖关系提出了基于单词共现的方法，可以借助网页中的锚节点文本挖掘隐藏的子主题，但这种方法不仅受限于查询query的质量，而且还需要外部资源的支撑，使得该方法的健壮性受到影响。


\subsection{基于结构的匹配方法}
基于结构的匹配方法（Structure-based Matching Methods）主要通过比较数据值的结构，把值和结构作为唯一的比较单元，并不考虑数据的语义信息，这种方法被大量用于半结构化数据，如XML文档。此类方法大致可分为两类：基于路径的匹配方法（Path-based）\ucite{Vinson2007An}和基于图结构的匹配方法（Graph-based）\ucite{Kailing2004Efficient}。

（1）基于路径的匹配方法是通过比较树结构中一条或多条路径进行匹配的。有的方法通过建立shingles，然后计算待比较实体对的shingles 的交集的集合大写与并集的集合大小的比例作为它们的相似度，而有的方法则使用Temporal Series 比较XML 文档结构的相似性，还有一些方法则是利用离散傅里叶变换（Discrete fourier transform）将时间序列从时间域转换为频率域进行比较的。

（2）基于图结构的匹配方法是通过计算实体间基于图的数据结构的相似度进行匹配的，有些方法利用基于树的编辑距离（tree-edit-distance）来表示XML 文档（实体）的相似性。而有的方法则使用边覆盖（Edge cover）的原理在创建的二分图上进行实体之间的匹配从而计算匹配相似性。还有一些方法则利用similarity flooding 计算带有标记边的图结构中节点的相似性，通过对节点相似度的组合获得匹配的相似性。但是，这些方法对实体的类型要求比较严格，实体必须是具有一定结构的该方法才能有效。如果实体是关系表中的记录，该类方法就失效了，因此存在适用范围小的问题。

\subsection{混合方式匹配方法}
混合方式匹配方法（Mixed Matching Methods）同时使用实体的值和结构，其中结构用于决定哪些值应该用于比较，此类方法大致可分为如下四类：（1）基于树的方法（tree-based）\ucite{Milano2006Structure}，通过结合扩展版的基于树的编辑距离方法和度量算法在树结构上进行相似性得分的计算；（2）基于AI 技术的方法\ucite{Yang2005Integrating}，该方法考虑如何将聚类算法和逆文档频率相结合应用于图结构中，利用迭代学习方法计算相似性；（3）基于路径的方法\ucite{Park2005An}，同时考虑数据值和与其相关的路径，并对这两个方面赋予不同的权重，然后计算总的相似性；（4） 基于IR 的方法\ucite{Carvalho2003Finding}，通常使用一些IR技术（如Vectorial model）实现匹配，通过分析将待比较实体的结构和数据值转化为向量形式，进而比较向量的相似度。

此外，随着实体数量的增加，实体匹配的效率会急剧下降，如何在获得较高的实体匹配准确性的同时，又能保证较高的匹配效率也是当前的研究热点。为了减少实体对的比较次数，近年来有许多高效的方法被提出。例如，一些工作将Q-gram 算法和倒排索引算法（Inverted Indices）\ucite{Christen2011A}结合，通过这两种方法可以提高匹配效率，以尽可能少的时间开销为目的进行实体匹配，但此类方法的适用性存在一定的问题，如当参与匹配的实体存在缺失值，此类方法将不再适。此外，还有一些基于前缀修剪（pre-fix based pruning）的方法\ucite{Wang2012Can}、 基于top-k model 的方法\ucite{Gal2006Managing}以及基于批量匹配（batch-based）方法\ucite{Chandel2006Efficient}，通过这些方法过滤那些“明显”不匹配的实体，可以减少匹配过程中的比较次数。

\section{本章小结}

本章首先介绍了数据质量问题的概念，介绍了同一数据源内的质量问题和多个数据源之间的质量问题，并给出了各类问题的解决方法。然后详细介绍了实体匹配的概念，并对实体匹配方法进行分类，分析了已有的各类方法的优缺点。

