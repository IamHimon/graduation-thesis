\chapter{相关理论及方法}
\label{chap:chap02}
本章首先介绍了深度学习用于自然语言处理的一般处理方案，介绍了深度学习在一些信息抽取领域的应用。然后介绍了信息抽取的相关概念及其定义，给出了文本记录分割与命名属性值识别问题定义。最后详细介绍了常见的处理方法，并分析了各类方法的优缺点。

\section{深度学习在自然语言处理上的应用}
最近几年来，深度学习架构和算法在诸如图像识别和语音处理上取得了突破性的进展，不断刷新各个榜单，应用场景越来越多，在工业界也有越来越多的落地产品，可以说深度学习的发展引导了这一波人工智能的热潮。图像和语音是自然界的产物，它的特征是一种更自然更丰富的表征，而深度神经网络的优势就在于它的特征抽取和特征组合能力，这是深度学习能在图像和语音上取得成功的原因。然后，人类的语言，属于人类文明创造的事物，不具备自然的表征能力，因此表达形式更主观，具有高度结构化、高抽象化、数据量相对小等特点，是一种更粗粒度的表现形式。因此，一开始，深度学习在自然语言处理领域的应用效果非常一般，随着算法的发展，特别是词向量的引入使得我们可以更好地将自然语言向量化，现在深度学习在越来越多的自然语言处理任务取得了最佳的表现。比如，深度神经网络模型在诸如文本分类，关系抽取，命名体识别，机器翻译等任务中的表现已经大大超越了传统方法，并且深度学习也在继续拓展它在自然语言理解上的应用。

在很长一段时间，自然语言处理的研究方法都是采用这些浅层的模型，来学习到非常高维且稀疏的特征表示。在传统的机器学习中，我们使用各种算法的基础是需要手工设计特征，因此特征工程是一个非常基础性的工作。只有当人们对特定领域的知识有非常透彻的理解时，才能构造出足够多、足够优质的特征。比如，对于命名题识别任务，当要识别地名和机构名，我们首先需要构造出图~\ref{chap:chap02:fig:NER_example}中这些特征：
\begin{figure}[!h]
  \centering
  \includegraphics[width=0.7\textwidth]{../figures/chap02/ner-features.jpg}\\
  \smallcaption{命名体识别任务特征列表}
  \label{chap:chap02:fig:NER_example}
\end{figure}
然后将特征喂给某个机器学习算法，比如线性分类器，分类器构造出目标函数，再通过凸优化策略不断调整模型的权重和偏置，使误差优化到最小，在这样的过程中为这些特征找到最合适的权重。这些人工设计的特征，常常需要定义很多，并且不可能是完整的，需要花费大量的时间去设计和验证。

然而在自然语言处理上，由于语言的特殊性，设计有价值的特征是非常困难的事情，想要从如此抽象的文本信息中抽取出有用的特征，必须经过不断的迭代和实验，这是非常耗时的。深度学习是一种端到端的模型，我们只需要提供输入，不需要做额外的特征工程，深度神经网络会自动的进行特征抽取、特征组合。我们知道，图像信息是以像素点为表示单位，声音是以声波作为表示单位，这些都较为底层的原始信号，可以直接数字化，作为深度神经网络的输入，进行端到端的模型训练。然而，一句文本却是以一个独立的单词组成的，虽然单词是独立的，但是组合成一起就成为了一句有意义的表述，如何将一句文本向量化表示，同时体现单词之间的联系，是一个重要的问题。近些年，基于稠密向量表示的深度神经网络在很多自然语言处理任务上取得了更好的表现，也就是词向量的横空出世，可以说是将深度学习引入到自然语言处理领域的重要转折点。下面介绍一下词向量以及词向量结合深度学习在自然语言中的应用。

\subsection{词嵌入技术}


\begin{figure}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{../figures/chap02/word2vec_sample.png}\\
  \smallcaption{词向量的样例}
  \label{chap:chap02:fig:word2vec_sample}
\end{figure}


想要将自然语言理解问题转化成机器学习问题，首先需要将自然语言数字化，就如何将图像、语音数字化相同。最直观的一种方式，也是最传统的方法――采用 one-hot 的编码，将每个单词表示成  $R^{|V| \times 1}$ 纬的向量，其中  $|V|$ 表示词典中词语的个数，对于第  $i$ 个词语的向量，只有  $i$ 下标处为1，其它纬都为0。比如'food', 'eat', 'laptop' 作为前三个词，则他们的 one-hot 向量表示为：
$$w^{food} = [1,0,0,0,\cdots,0] \qquad w^{eat} = [0,1,0,0,\cdots,0] \qquad w^{laptop} = [0,0,1,0,\cdots,0] \qquad \cdots$$

这种向量的表示形式，实现了将自然语言数字化的目的，在实际的应用中，配以Hash的处理，再结合一些机器学习算法就可以很好的解决自然语言处理领域的各种任务了。但是它存在两个最主要的问题：

(1)纬度灾难~\cite{bengio2003neural}。当某一语料词典数目过大，这个向量的维度会变得很大，数据会变得特别稀疏，导致统计语言模型会出现很多为零的条件概率，这需要花费大量的精力来处理零概率的问题。这导致在训练模型进行矩阵时计算会非常困难，特别是应用在深度学习模型上，容易导致纬度灾难。

(2)不能很好地刻画单词之间的相似性。也就是常常说的'词汇鸿沟',对于两个词性或者语义上有关联的词语，他们的向量表示应该能体现出一些关联性和区分性，但是使用 one-hot 编码无法实现这个目的。比如对于 'food','eat','laptop'这三个词：
$$(w^{food})^{T} \times w^{eat} \quad = \quad (w^{eat})^{T} \times w^{laptop} \quad = \quad 0$$
他们的向量相乘结果是相同的，且都等于0，这就无法体现'food'和'eat'之间的相关性，也不能体现这三者的区分性了。

因此我们想要去学习出一种使用低维空间的分散式表示来向量化一个词语，同时又需要将相似性和关联性融入到这个向量中，这就是词嵌入技术，需要我们用语言模型来训练这个词向量，因此词嵌入技术的原理依据就是具有相似含义的词语往往出现在相似的语境之中。使用神经网络训练语言模型在~\cite{bengio2003neural} 中被首次提出，作者阐述了传统基于统计的语言模型的主要问题，并说明了使用神经网络训练语言模型的优势。词嵌入技术得到革命性的发展是~\cite{mikolov2013distributed}提出的CBOW模型和Skim-gram 模型，也就是著名的词向量\textbf{word2vec}。

构建词向量，首先对词典中的所有单词进行one-hot编码，词典中词语数目为 $V$。CBOW 模型是使用周围的 $2C$ 个词语来预测目标词语，比如对于一句话"狗 喜欢 吃 熟的 骨头 "，我们可以将$ \{' 狗'， '喜欢', '熟的', '骨头'\} $ 看做周围语境(两边各 $C$ 个词语)，来预测和生成目标词语$\{ '吃'\}$，而Skim-gram 模型是跟这个思路相反的，它使用目标词语来预测它周围的 $2C$ 个词语。CBOW 模型的简单形式如图~\ref{chap:chap02:fig:CBOW} 所示,这里只考虑了句子中的一个词语。CBOW 模型本质上就是一个三层的全连接神经网络(包含一个隐藏层)，输入层和输出层都具有 $V$ 个神经元结点, 隐藏层具有 $N$ 个神经元结点，往往 $N$ 要大大小于$V$。则CBOW模型的输入层就是通过one-hot 编码的 $V$ 维度向量，这样one-hot 编码的向量的每一维的值便可与输入层每个神经元结点对应起来。
更一般的形式如图~\ref{chap:chap02:fig:CBOW_general} 所示，模型的输入就是目标词语的周围语境，即 $C$ 个词语。假设输入的 one-hot 词向量表示为 $x^{(i)}$，模型的输出表示为 $y^{i}$，这个模型的唯一输出 $y^{i}$ 也就是我们的目标词语 $y$, 同样是使用one-hot编码的向量。另外，我们需要创造两个矩阵，也是这么模型输入层到隐层和隐层到输出层的权重矩阵， $W^{1} \in \mathbb{R}^{N \times |V|}$ 和  $W^{2} \in \mathbb{R}^{|V| \times N}$ , 这里 $N$ 就是隐层神经元数目，也表示最终词向量的纬度空间。

这个语言模型的训练过程可以分解为如下几个步骤， 对于第$i$个目标词语：

(1)首先生成 $2C$ 大小上下文词语的one-hot向量，记做 $( x^{i-C}, \cdots , x^{i-1}, x^{i+1}, \cdots , x^{i + C})$

(2)对于这个上下文，计算其中所有词语的嵌入向量 $( u^{i-C} = W^{1}x^{i-C},  u^{i-C+1} = W^{1}x^{i-C+1}, \cdots ,u^{i+C} = W^{1}x^{i+C} )$

(3)计算这些向量的平均向量 $h = \frac{u^{i-C}+u^{i-C+1}+\cdots+u^{i+C}}{2C}$

(4)计算输出层的得分向量 $z = W^{2}h$

(5)使用softmax将输出层得分向量变成归一化的概率值 $ \widehat{y} = softmax(z)$

(6)使用这个输出概率值与真正的目标词语 $y$ 比较，通过计算交叉熵 $H(\widehat{y^{i}}, y^{i})$ 来定义损失函数，再通过梯度下降一步一步调整权重矩阵，以此来优化这个模型。

而最终训练处的模型的输入层到隐藏层的权重矩阵 $W^{1}$ 才是我们需要的词向量表示，具体来讲，第 $i$ 个词语的词向量就是 $W^{1}$ 的第 $i$ 列的向量 $w^{i} \in \mathbb{R} ^{N \times 1}$ 。图~\ref{chap:chap02:fig:word2vec_sample}展示了词向量样例，可以看到可以体现词语之间的相似性。当然这只是最基础的词向量构造方法，还有很多版本的词向量构造思路，最近基于字符级别的字符向量可以捕获到更多句法级别的信息，展现了巨大的应用潜力。



\begin{figure}[!h]
  \centering
  \includegraphics[width=0.6\textwidth]{../figures/chap02/CBOW.png}\\
  \smallcaption{CBOW模型的简单形式}
  \label{chap:chap02:fig:CBOW}
\end{figure}


\begin{figure}[!h]
  \centering
  \includegraphics[width=0.6\textwidth]{../figures/chap02/CBOW_general.png}\\
  \smallcaption{CBOW模型的一般形式}
  \label{chap:chap02:fig:CBOW_general}
\end{figure}

\subsection{各种深度学习模型的使用}
理解了词向量的构造原理，就解决了深度学习应用在自然语言处理上最基本的问题――输入的表示。剩下的就是根据具体问题，如何选择模型，如何设计解决方案的问题了。有很多深度学习的模型可供选择，比如深度神经网络DNN, 卷积神经网络CNN，循环神经网络RNN，长短期记忆神经网络LSTM，GRU，递归神经网络，基于注意力机制的深度神经网络模型，Encoder-Decoder模型，记忆网络模型等等。

基于深度学习模型的方法已经在很多自然语言理解领域取得了更好的表现，比如,
在文本分类问题上，文章~\cite{kim2014convolutional,kalchbrenner2014convolutional}提出了基于卷积神经网络的模型，文章~\cite{kim2014convolutional}使用比较普通的卷积神经网络结构，文章~\cite{kalchbrenner2014convolutional}中使用一种多层的交织最大池化层和卷积层的网络结构。文章~\cite{socher2013recursive, tai2015improved} 都使用基于循环神经网络模型，两者的对比体现了LSTM 相比较普通RNN 的优势，并且文章~\cite{tai2015improved} 使用的是一种树形长短期记忆网络(Tree-LSTM)，证明了树形LSTM 相比于双向的LSTM 的优势，这表明Tree-LSTM 能够更好的捕捉到自然语言的句法信息。文章 ~\cite{kumar2016ask} 使用的基于记忆网络模型目前为止取得了最优的表现。

在机器翻译任务上，文章~\cite{sutskever2014sequence}使用一个四层的 seq2seq 模型，使用LSTM， 然后用这个模型对从传统SMT方法产生的1000个候选翻译重新排序。文章~\cite{wu2016google}使用一个带有 8 个编码器和 8 个解码器的深度 LSTM 网络组成，并使用了注意机制（attention）和残差连接（residual connections）。最近，~\cite{gehring2017convolutional}提出了一种基于CNN 的seq2seq模型，输入的每个词语通过CNN由并行方式组成的注意力架构计算出其表示，另外，解码状态需要同时考虑之前已经产生的结果。~\cite{vaswani2017attention} 中不在是使用RNN 或者CNN来作为encoder-decoder的模型基础，只使用注意力机制处理序列模型相关的问题，比如机器翻译，这样可以高度并行的工作，在提升翻译性能的同时也大大提升了翻译速度。

在信息抽取任务上，文章~\cite{nguyen2015relation}提出的基于卷积神经网络的模型，特殊之处在于作者将位置信息与词向量组合作为模型的输入矩阵，在关系抽取上取得了非常好的表现。文章~\cite{zheng2017joint}利用共享神经网络底层表示来进行联合学习，同时进行实体识别和实体的关系抽取。具体来讲，对于输入的句子共享词向量输入层，通过双向LSTM对输入进行编码，然后分别使用LSTM 进行命名体识别，使用CNN进行关系分类，这种方法可以很好的解决传统的流水线方法的很多弊端。

以上列出的一些自然语言处理的几个领域，使用深度学习已经取得了突破性的进展，在其他一些领域，比如POS标注，语义解析，问答系统等，使用深度学习模型也都是目前最好的方法。当然也有很多工作是使用深度学习结合传统的机器学习算法实现的，深度神经网络具有它天生的优势，将会有更多的应用场景。

\section{文本记录分割与命名属性值识别概念以及现有的方法}
文本记录分割与命名属性值识别是一个经常被研究的问题，用来分割一段连续文本，然后抽取出隐含在这段文本中的各属性值。比如，对于这样一段连续文本， ' Mercedes-Benz E250 Auto, \$84888, Obsidian Black, 7speed Sports Automatic. 2 doors 4 seat Coupe, 4 cylinder Petrol TurboIntercooled2.0L;6L/100km'，它有这样几个特点：(1)文本中包含若干属性项，且各个属性值之间是语义相关但不是语法相关的，(2)各属性值不通过固定标点分割，(3)通常属性值顺序不固定，(4)同一语料中不同记录所包含的属性值数目不一定相同，(5)文本中没有噪音数据。这些半结构化文本出现在很多场景下，比如商品信息描述，引文信息，个人信息，地址信息，分类广告，商品评价等等。正因为这种文本格式在互联网上广泛的出现，且文本中包含了大量的有价值的信息，研究如何抽取这类文本在学术界和产业界都引起了大量的关注。

对于上面的例子，一个正确的分割结果如图~\ref{chap:chap03:fig:framework}所示，这个任务主要解决两个问题，一是正确的分割出各属性值，二是给各个文本块打上正确的标签。各属性值之间虽然是语义相关的，但是因为其在语法上没有任何关联，所组成的一段文本不是一个常规的自然语言，不符合语法规则，这给这个任务带来了巨大的挑战，我们无法单纯从内容或者语法上来解觉这个问题。解决这个问题的方法大体可以分为两种：(1)使用基于图的概率模型,比如隐马尔可夫模型(HMM)和随机向量场模型(CRF)实现的监督式方法，(2)借助知识库实现的无监督的方法。

\subsection{基于图的概率模型}
因为文本记录分割与命名属性值识别可以看做是一种序列标注问题，使用机器学习技术，特别是一些基于图的概率模型，比如隐马尔可夫模型(HMM)和随机向量场模型(CRF)，展现了很好的效果。当使用机器学习技术时，既有使用人工标注语料的监督式模型也有非监督式模型。文章\cite{freitag2000information} 首次提出了采用基于图的概率模型来解决信息抽取问题，作者训练若干个独立的隐马尔可夫模型来识别各个属性值，这种方法被扩展成一个DATAMOLD抽取工具
\cite{borkar2001automatic}，在这种方法中，各属性的隐马尔可夫模型，也被称作内部隐马尔可夫模型，被包裹起来组成一个外部隐马尔可夫模型，这些外部的隐马尔可夫模型的目的是来刻画目标文本中各属性值的序列状态。内部隐马尔可夫模型和外部隐马尔科夫模型使用标注数据分别训练，此方法在两个数据集上展现了非常出色的效果。

隐马尔科夫模型是一种生成式序列模型，它设计出一种交叉概率来匹配目标文段和标注序列，但是这种模型不擅长表达多层交叉特征和处理较长依赖的序列。后来，基于随机向量场模型的方法被提出来解决此类问题\cite{lafferty2001conditional, peng2006information}。相比较隐马尔科夫模型，随机向量场模型具有更强的推理能力，能够使用复杂的，重复的，非独立的特征来训练和推理，因此它能够更充分地利用上下文信息，并且不同于隐马尔可夫模型，它可以使用其他的外部特征来获取更丰富的信息。文章\cite{lafferty2001conditional}中分析，对于不同的输入序列，在隐层状态具有不同的状态转移和发射概率，随机向量场模型更适合用来对这种情况进行建模。

尽管这些基于图的概率模型的方法表现非常好，但是上面这些方法都是监督式模型，需要我们去标注大量的训练数据，这个工作的花费往往非常昂贵。还有一些情况，训练数据根本就难以获得。为了解决上述的这些问题，出现了一些使用事先存在的知识库来辅助训练的方法
\cite{agichtein2004mining, mansuri2006integrating, zhao2008exploiting, cortez2007flux}，这些方法使用已经收集的大量的语料，无需使用人力或者只需要一点点人力，就可以训练出抽取模型。这种方法的策略是使用这些已经存在的数据集中的属性值训练出模型，然后在新的输入文本中来识别各属性值。\cite{mansuri2006integrating}提出的方法基于随机向量场模型来抽取属性值，首先先从数据集中学习到一些内容相关的特征，再用一些人工标注的数据来学习结构相关的特征，然后结合两者完成抽取过程。
\cite{agichtein2004mining, zhao2008exploiting}提出的方法能够仅仅依赖数据集来训练模型，然后再使用这个模型在分割的文本中识别属性值，这种方法不需要使用人工标注的训练集，一旦属性值被识别到，整个抽取过程就算完成了。但是，这个方法基于一个假设：一个数据集中所有文本记录中的属性值都服从同一个顺序，这个顺序就是先从一批测试样例中学习到的。这两篇文章的不同之处在于，\cite{agichtein2004mining}使用隐马尔可夫模型，
\cite{zhao2008exploiting} 使用随机向量场模型。但是，这种策略虽然避免了人工的依赖，但是因为它的条件限制在具有固定属性值顺序的语料上，所以在现实中的使用受到限制。
\cite{mansuri2006integrating}中提出的方法能够解决不同顺序的问题，但是却需要依赖人力的帮助来学习结构相关的特征，增加一些人工的依赖和花费，以便应用在更多的实际场景中，是一种折中的方法。

\subsection{基于知识库实现无监督的抽取模型}
除了上面讨论的基于图的概率模型，也有一些其他的借助知识库实现的非监督式模型。
\cite{michelson2007unsupervised}提出了一种不一样的思路，首先，作者需要构建一种特殊的文本库，使用简单的空间向量模型计算输入文本和文本库中文本集的相似度，系统可以自动找到对于指定抽取任务最相关的文本集。既然文本集已经确定，系统再借助已经定义好的文本相似度矩阵，例如 Jaro-Winkler 和 Smith-Waterman， 通过调整一个阈值抽取出有用的信息。这个思路不同于上述的基于机器学习模型的算法，上述方法需要先学习到一些内容相关的特征再做抽取，这种方法是使用一个事先定义好的文本相似度函数来直接做抽取，但是这种方法需要大量的文本集做支撑。

ONDUX\cite{cortez2010ondux}是另一种不同的思路实现了无监督的抽取模型。这种方法依赖的知识库与之前的都有所不同，它存储的内容不再是整条文本，而是将各个属性值分开来存储构建知识库。此方法的整个过程大体可以分为三步，第一步，根据词语在知识库中的共现来分割输入文本，得到若干个文本块。第二步，作者定义了一些匹配函数，比如匹配文本类属性值函数，匹配数字类属性值函数，匹配URL和e-mail类属性值函数，使用这些匹配函数直接去匹配第一步中生成的文本块。第三步，在执行抽取的过程中，作者构建了一个位置和序列模型 PSM ，这个模型用来刻画文本的结构特征，然后使用 PSM 模型最后再对标注结果进行补充和修改。这种方法主要的瓶颈在两点，第一，匹配函数的表现力不足，第二， PSM 模型不能够充分的描述文本的结构特征，它只能单向的描述序列和位置的转移和分布特征，虽然可以提供很大的帮助，但这个模型作为最终修改的参考还不够完美。


\section{本章小结}

本章第一部分，首先介绍了深度学习在自然语言理解上的应用，详细地介绍了词向量的生成原理和应用优势，然后列举了一些深度学习模型，并简要介绍了在一些自然语言理解领域使用深度学习模型的方法。第二部分，首先给出了文本记录分割与命名属性值识别任务的概念，然后介绍了两种主要的解决思路，分别列举了一些典型的方法，并分析了这些方法的优缺点。


