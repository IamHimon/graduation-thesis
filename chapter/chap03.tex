% 修改表格与表头的间隔
\newcommand{\topcaption}{%
\setlength{\abovecaptionskip}{0pt}%
\setlength{\belowcaptionskip}{8pt}%
\caption}   % 表格上标题修改

\chapter{基于卷积神经网络的贪婪式概率标注算法}
\label{chap:chap03}
本章首先给出了基于卷积神经网络的贪婪式概率标注算法(简称为 $CNN-IETS$ )概述，然后给出了分割式信息抽取的正式定义，并介绍了知识库的结构。接下来详细介绍了算法每一部分的内容，最后给出了详细的实验结果来证明算法的效果和表现。


\section{算法概述}

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{../figures/chap03/framework.pdf}\\
  \smallcaption{模型的整体结构}
  \label{chap:chap03:fig:framework}
\end{figure}

这一章，我们提出了一种非监督式概率标注算法，它基于深度卷积神经网络(CNN)来构建模型，借助提前构造的知识库来克服需要人工标注语料的难题。卷积神经网络是一种端到端的模型，先天具有抽取特征和组合特征的优势，使用深度卷积神经网络能够自动得选择出高质量的特征来更好的刻画各个属性值。而这个知识库有两个作用，第一个作用是用来训练我们的卷积神经网络模型分类器，第二个作用是协助生成文本段，再输入到我们的分类器中。具体一点讲，如图~\ref{chap:chap03:fig:framework}，输入一个文本,我们首先根据词语在知识库中的共现来初始分隔这条文本，形成若干个文本段。对于这些文本段，如果它在某个属性组中出现的频率比较高而在其他属性组中出现的频率比较低，并且得分高于某个阈值，它就会被标注成这个属性。然后根据这些初始的分割状态和标注结果，哪些没有标注的文本段会被输入到下一步的CNN分类器中。在下一步，我们并不是直接接受CNN分类器的输出结果，我们根据CNN分类器的输出概率发明了一种\emph{贪婪式概率标注算法}，这个算法会从全局考虑分割和标注状态来为输入文本执行最合理的分割和标注方式。对于一些有上面过程产生的漏标注和错误标注的情形，我们提出了一种\emph{双向位置与序列模型}，进一步对标注结果进行调整和改善。

为了构建我们的卷积神经网络模型，我们使用了一组全面的语义特征和句法特征加入到模型的输入中，这些特征都是直接从我们的知识库中构造的，这样我们的模型就可以充分利用这些特征来标注每一个独立的文本段。虽然，我们的模型可以直接提供一个标注的结果，但是这些结果仅仅是面对单独的文本段，没有考虑在同一个输入文本中各文本段之间的内在关系，这也不能找到最佳的分割和标注结果。因此，我们需要从全局考虑分割和标注状态来执行概率标注。因此，最理想的标注方式是，我们依次标注每个文本块来最大化所有标注结果的概率和，但在这个过程中，必须满足一个前提条件，即每个属性值只能包含连续的文本段。我们首先证明了这是一个NP-hard的问题，然后提出了一种贪婪式概率标注算法，此算法能够有效的实现最佳标注方式。

使用上述的基于卷积神经网络的贪婪式概率标注算法，大部分的输入文本都能够被正确分割和标注了，但是还是会出现一些漏标注和错误标注问题。为了解决此类问题，我们在执行抽取的过程中了权衡考虑了已经标注文本段的位置和序列信息，相比较文章\ucite{cortez2010ondux}中提出的单向序列模型，我们的是双向模型，从前向和后向同时考虑序列关系，因此能够减少更多的标注错误并提供更大的帮助。

\section{基于卷积神经网络的贪婪式概率标注算法}
在这一节，我们首先给出分割式信息抽取任务的定义，然后详细介绍借助知识库我卷积神经网络构建的贪婪式概率标注算法(CNN-IETS)。

\begin{definition}
{\bf (分割式信息抽取任务)}
对于某个领域的属性集 $\mathbb{A}$, 假设输入文本 $I$ 包含了一些属性集 $\mathbb{A}$中的属性值, 分割式信息抽取任务目的是分割 $I$ 成一些列的文本段 $S=\{s_1, s_2, ..., s_m\}$, 然后使用一个属性值 $A_i \in \mathbb{A}$ 来标注每一个文本段 $s_i$ ($1\leq i \leq m$), 这里 $s_i$ 数属性 $A_i$中的一个属性值.
\end{definition}

\noindent \underline{\em 知识库:}
我们使用实现存在的语料库来构建一个固定领域的知识库(简称为KB)，就像\cite{cortez2010ondux, zhao2008exploiting}中做的一样。简单讲，知识库(KB)包含一组特定的属性以及对应的属性值，我们记做 KB=$\{ <A_1, V_1>, <A_2, V_2>, ..., <A_n, V_n> \}$ ，这里每一个属性$A_i$ ($1\leq i \leq n$)都是这个领域属性集  $\mathbb{A}$ 中的一个独立属性， $V_i$ 是一组词语，是在属性$a_i$中出现的典型的、合理的值，并且记录每个属性值在记录中的位置 $\{(v_{i,1}, PS_{i,1}), (v_{i,2}, PS_{i,2}$ $), ..., (v_{i, n_i}, PS_{i,n_i}) \}$，这里$PS_{i, k}$ 是一组位置相关的数值，来表示这个属性值 $v_{i, k}$ 出现在不同记录中的位置属性。一个简化的知识库如图~\ref{chap:chap03:fig:KB}所示，为了方便理解，这里我们只是使用了一些简化的单词而不是使用原始的属性名。

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{../figures/chap03/KB.pdf}\\
  \smallcaption{一个简化的知识库}
  \label{chap:chap03:fig:KB}
\end{figure}

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{../figures/chap03/labelExample.pdf}\\
  \smallcaption{基于知识库的初始分割和标注例子}
  \label{chap:chap03:fig:labelExample}
\end{figure}


\subsection{基于知识库的初始分割和标注}
\label{KB-based Segmentation and Labelling}
这一模块的主要目的是识别出输入文本中所谓的\emph{锚点块}(Anchor Blocks)，就是对一些文本段，我们会根据知识库计算出来一个标注可信度，当这个可信度大于某个阈值时，便可以直接将这个文本段标注成某一个属性。然后，这些识别出来的\emph{锚点块}将被视作一个基础锚点，执行后面步骤中的分割和标注。

\begin{definition}
{\bf (锚点块)}
给定一个输入文本 $I$, 如果它满足下面的两个条件，$I$ 中的一个文本段 $b$ 才可以被标注成具有属性 $A$ 的锚点块：
%
1)没有任何一个其他属性 $A' \in \mathbb{A}$  满足 $p(b, A') >=p(b, A)$ ， 这里 $p(b, A)$ 是我们定义的一个方程用来计算一个文本段 $b$ 属于属性 $A$ 的概率。
%
2) $p(b, A) > \theta$，这个阈值 $\theta$ 是我们自己定义的，用来提高锚点块的标注质量。
\end{definition}

假设一个文本段 $b$ 出现在知识库中的 $m$ 个不同属性$\{(A_1, A_2, ..., A_m\}$ 的属性值里面，这里$A_i \in \mathbb{A}$ ($1\leq i \leq m$)表示 $b$ 的一个可能的属性。对于每一个 $A_i$ ， 假设 $b$ 出现在一组不同的属性值中，并且具有不同的出现频率，我们记做  $\{(v_{i,1}, f_{i,1}), (v_{i,2}, f_{i,2}), ..., $ $(v_{i,r_i}, f_{i, r_i})\}$ ，假设所有属性值都是互相独立的，则我们可以通过下面的公式，计算出这个文本段 $b$ 属于属性 $A_i$ 的概率为：

\begin{equation}
\label{eq1}
p(b, A_i) = \frac{1+\sum_{1\leq j \leq r_i}\sum_{1\leq x \leq f_{i, j}}{\frac{1}{x}}}{1+\sum_{1 \leq k \leq m}\sum_{1\leq j \leq r_k}\sum_{1\leq x \leq f_{k, j}}{\frac{1}{x}}}
\end{equation}

这里，我们在分母使用加1的操作是为了防止分母出现为零的情况。 有了这个公式和我们定义的阈值 $\theta$ ，我们便能够在输入文本 $I$ 中标注出一些\emph{锚点块}了。因为，我们的方法的抽取质量和效率是对阈值 $\theta$ 敏感的，所以我们会通过后面的实验部分找到一个合适的阈值。

注意，一个单独的属性值可能被分割成了两个或更多个文本块，比如图~\ref{chap:chap03:fig:labelExample}中，属性 $D$ 便有两个文本块 $d_1$, $d_3$。考虑到在一个输入文本中每个属性可能有一个属性值，所以，我们可以将这两个\emph{锚点块}和他们之间所有的词语都合并到一起。这里可能出现的一个冲突是，在一个属性的两个\emph{锚点块}之间出现了另一个属性的\emph{锚点块}，通常，一个合适的阈值 $\theta$ 便可以避免出现这种冲突。但是，如果这种情况仍然存在，我们可以通过弃掉冲突\emph{锚点块}中得分更低的那个来解决这个问题。

例子一：给定图 ~\ref{chap:chap03:fig:KB} 中的知识库， 对于一个图~\ref{chap:chap03:fig:labelExample}(1)中输入文本， 我们能够识别出下面这些\emph{锚点块}: $a_2$, $b_1 b_2$, $c_1$, $d_1$, $d_3$ 以及 $f_2$ ，就是图~\ref{chap:chap03:fig:labelExample}(2)中绿色方格这些文本段。另外, 既然 $d_1$ 和 $d_3$ 都是属性 $D$ 的\emph{锚点块}, 我们合并这三个块 $d_1 d_2 d_3$ ，并共同标注为属性 $D$ ，如图~\ref{chap:chap03:fig:labelExample}(3)所示。


\subsection{基于卷积神经网络模型的构建和文本块的标注}
为了处理一些未标注的文本段，比如上述例子中第一步产生的$a_1$， $a_3$ 和 $c_2$，我们需要执行一个基于卷积神经网络模型分类结果的概率标注。简要来讲，我们首先使用知识库训练一个卷积神经网络分类器(CNN分类器)，假设 $|\mathbb{A}|$ 表示给定领域中属性的个数，然后对于每一个输入文本 $s$ ，我们的CNN分类器会生成一个 $|\mathbb{A}|$-维的输出向量 $P(s)=[\rho_1, \rho_2, ..., \rho_{|\mathbb{A}|}]$， 这里每一个 $\rho_{i}$ ($1\leq i \leq |\mathbb{A}|$)表示$s$属于属性 $A_i \in \mathbb{A}$ 的概率。对于每一个未标注的文本段，我们使用我们的CNN模型来决定它属于哪一个属性，然后综合考虑所有未标注文本段的CNN模型输出的概率结果和那些\emph{锚点块}，得到整个输入文本的最佳分割和标注结果。下面，我们解释如何构建CNN模型，并且介绍如何找到最佳的标注方式。

\begin{figure}[!h]
  \centering
  \includegraphics[width=1\textwidth]{../figures/chap03/CNN_des.pdf}\\
  \smallcaption{CNN分类模型的架构图}
  \label{chap:chap03:fig:CNN_des}
\end{figure}

\subsubsection{构建CNN分类模型}
图~\ref{chap:chap03:fig:CNN_des}给出了我们构建的CNN分类模型的架构图，它的输入时一个文本段，输出一个关于所有可能的属性的概率向量。整体来看，我们的CNN模型包括特征输入层，嵌入层，卷基层，最大值池化层，全连接层和最后的Softmax分类器层。特征输入层接受一个输入文本段的特征矩阵，这个特征矩阵包含了丰富的句法和语义特征。嵌入层负责将这些特征值对应到相应的特征向量然后将他们合并在一起。然后我们在特征矩阵上执行卷积操作来提取关于输入文本段的特征，在这一步我们使用了不同大小的过滤核来在输入文本段上尽可能多的覆盖n-gram的文本范围，这样我们就可以生成多层次的，且不同尺寸的特征矩阵。然后，我们再使用最大值池化来对卷积层的结果进行标准化，使其最终都成为相同形状的特征矩阵，且这一步也是进一步抽取特征和降维的操作。池化层得到的特征矩阵再通过一个全连接层，就是一个普通的神经网络，做进一步的特征组合和推理。最终，我们使用Softmax分类器作为我们的输出层，为每一个输入文本段产生分类结果。更多的细节，可以去参考~\cite{sahu2016relation, kim2014convolutional}。

\smallskip
\noindent \underline{\it 特征:}我们为每个输入文本段都构造了一些列丰富的语义特征，包括一些合成的特征。对于文本段中的每一个单词，他的特征矩阵中包括了，提前训练的词向量 Word2vec， 文本段的位置信息，文本段在输入文本中的位置信息，词性标注信息(POS)，文本段长度，以及一种人为构造的归一化类别概率。下面给出每一个特征的详细介绍：

\begin{enumerate}[1)]\setlength{\itemsep}{-1pt}
\item {\bf Word2Vec:} 对于词向量，我们直接使用文章~\cite{th2015evaluating}中用~\cite{mikolov2013distributed}词向量模型在Pubmed文章上提前训练好的词向量，这个词向量模型是由Tomas Mikolov在Google发明，我们在之前的章节有详细介绍。对于中文实验集，我们在自己的中文语料上使用Google的word2vec工具~\footnote{ https://code.google.com/archive/p/word2vec/}训练了一个词向量。

\item {\bf 在文本段中位置特征:} 这个位置特征是指一个单词在输入文本段中的位置，我们之所以使用这个特征，是因为我们观察到，在一些特定的属性值中，一些特殊的词语会出现在相同的位置上。

\item {\bf 在输入文本中的位置特征:} 这个位置特征是指一个单词在原始输入文本中的位置，我们选择这个特征是因为一个单词在文本中的所在位置往往与它属于哪个属性有密切的关联，这个一个比较容易想到的特征项。

\item {\bf 文本段的长度:} 也就是文本段中单词的个数，使用这个特征是因为不同属性的属性值的平均长度往往是有差别的，这说明，文本段的长度是一个非常有区分性的特征。

\item {\bf 语法和词性标注特征(POS):} 构造POS特征的过程，对一句话中的每个单词，依赖这个单词的词性定义和上下文来给它标注一个标签，用以说明这个单词在这句话中的角色。这里，我们使用GENIA tagger~\footnote{ http://www.nactem.ac.uk/GENIA/tagger/}来得到每个词的POS特征。

\item {\bf 归一化类别概率:} 我们同样使用公式~\ref{eq1}来计算一个概率向量$\vec{cp}$，向量每一维的值表示一个单词属于某个属性的概率。

\end{enumerate}

\subsubsection{构建CNN分类模型}
虽然CNN分类器可以直接输出一个输入文本段属于各个属性的概率值，因为已经存在标注了的\emph{锚点块}的位置，所以每个输入文本段只会有几个可能的候选属性，因此，对于CNN分类器的结果，有些概率中是用不着的。例如，在图~\ref{chap:chap03:fig:labelExample}(3)中，文本段 $a_3$ 只可能属于属性 $A$， $B$ 和 $E$
中的一个，因为我们已经有了属性为 $C$，$D$ 和 $F$ 的\emph{锚点块}，且这些\emph{锚点块}都在文本段$a_3$的非邻接位置。我们在图~\ref{chap:chap03:fig:candidate-Labels}列出了其他文本段的所有候选属性列表，其中已经得到的\emph{锚点块}只有一个候选属性。

对一个文本段 $s$ ，另 $\mathbf{A}_c(s)$ 表示一组候选属性，$P(s)=[\rho_1, \rho_2, ..., \rho_{|\mathbb{A}|}]$ 表示$s$ 的CNN模型的输出结果，则文本段$s$ 属于一个候选属性 $A \in \mathbf{A}_c(s)$ 的概率用下面的公式计算得到：
\begin{equation}
\label{eq:2}
p(s, A)=\frac{\rho_{i(A)}}{\sum_{A' \in \mathbf{A}_c(s)}{\rho_{i(A')}}}
\end{equation}
这里 $i(A)$ 表示属性 $A$ 在属性列 $\mathbb{A}$ 中的下标。对于每一个输入文本段，我们都可以使用公式~\ref{eq:2}计算得到它属于每个候选属性的概率值。

例子二：图~\ref{chap:chap03:fig:candidate-Labels}中列出了例子一基于CNN模型得到的各未标注文本段的属于各候选属性的概率值。对于文本段 $a_3$ ，假设它原始的CNN模型输出的概率向量是  $[0.567, $ $0.067, 0.199, 0.133, 0.033]$ ， 因为 $a_3$ 只可能属于属性 $A$，$B$ 或者 $E$ 。因此，我们能计算出文本段 $a_3$ 属于 $A$ 的概率为：$p(a_3, A) = \frac{0.567}{0.567+0.067+0.033} = 0.85$ ，相同的，我们可以计算出属于 $B$ 和 $E$ 的概率 $p(a_3, B)=0.1$ 和 $p(a_3, E)=0.05$。


\begin{figure}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{../figures/chap03/candidate-Labels.pdf}\\
  \smallcaption{例子中文本段的候选属性值和基于CNN模型的变换概率值}
  \label{chap:chap03:fig:candidate-Labels}
\end{figure}


\subsection{输入文本的贪婪式概率标注}
\label{Probabilistic Labelling to Input Texts}

尽管我们的CNN模型能够直接给出输入文本段属于各属性的概率，但是这种标注方式只是单独的考虑了的这一个文本段，没有综合考虑这个文本段与输入文本中其他文本段的关系，因此这种方式可能不能够找到最佳的分割和标注方式。因此，我们在执行每一个概率标注的时候需要整体考虑整个文本的分割和标注状态，事实上，我们期待去找了一种最佳的标注方式来最大化整体的标注概率值得分，而这个得分就是输入文本中所有文本段属于对应属性的概率之和，但是这个过程需要遵循一个条件，这个输入文本中每个属性值只能由位置连续的文本段组成，我们把这个问题定义为\emph{基于CNN的受限分割式信息抽取}(Constrained CNN-IETS Problem)：

\begin{definition}
({\bf Constrained CNN-IETS Problem}) 给定某个领域属性集 $\mathbb{A}$ 以及一个输入文本 $I$ 来做分割式信息抽取任务,，假设 $S(I)=\{s_1, s_2, ..., $ $s_m\}$ 表示从 $I$ 中分割出来的文本段， 其中 $s_1+s_2+...s_m=I$, 并且 $s_i \cap s_j = \emptyset$ ($i \neq j$)，
%
每一个文本段 $s_i \in S$ 都有一组候选属性标签 $\mathbf{A}_c(s)$，则\emph{基于CNN的受限的分割式信息抽取}任务的目的是最大化下面这个方程式：

\begin{equation}
\label{eq:3}
Prob(I, \mathbb{A}) = \sum_{s \in S(I)}{p(s, A_s)}
\end{equation}
, 这里 $A_s \in \mathbf{A}_c(s)$ 是 $s$ 的一个可选属性标签， 所谓\emph{受限}是指，对于任何不相邻的文本段 $s_a \in S(I)$ 和 $s_b \in S(I)$，他们的可选属性标签 $A_{s_a} \neq A_{s_b}$ ，除非他们之间的所有文本段都具有相同的可选属性标签。

\end{definition}

但是，这中基于CNN模型结果的受限的分割式信息抽取问题(Constrained CNN-IETS Problem)是一个NP-hard问题，我们给出下面的定义：

\begin{theorem}
\label{theorem:1}
基于CNN的受限的分割式信息抽取问题是一个NP-hard问题。
\end{theorem}

\begin{proof}
首先，我们假设输入文本中一个没有任何\emph{锚点块}的属性称作\emph{自由属性}(Free-Attribute)，如果在这个输入文本中存在属性值，则它可能出现在这段文本中任何没有标记的位置上。假设，这个输入文本中有 $n$ 个自由属性，我们可以将我们的基于CNN的受限的分割式信息抽取问题约简地看做是旅行商问题(Travelling SalesMan problem，TSP)。另 $V$ 表示一组城市, $A=\{(r,s):r,s\in V\}$ 表示城市之间的路径，$C(r, s)=C(s, r)$ 表示城市 $r$ 和城市 $s$ 之间的路程费用，旅行商问题(TSP)是最基本的路线规划问题，就是找到最小的路程总费用而访问完所有城市。

我们可以将我们的问题简化成为旅行商问题(TSP)：将我们的问题中的 $n$ 个自由属性看做旅行商问题中的那一组城市 $V$ ，将公式~\ref{eq:3}中的 $p(s, A_s)$ 看做是与边 $(A_c, A_s)$ 相关的路费计算方式，当且仅当 $A_c$ 是第 $i$ 次出发的城市。然后，我们的问题就是找到最大的路程总费用而访问完所有城市。因此，我们可以发现，基于CNN的受限的分割式信息抽取问题跟旅行商问题一样，在旅行商问题中需要去全排列列出来所有可能的路线，然后找到最小化花费的旅行路线，我们的是要全排列列出所有可能的文本段组合方式，找到最大概率值得标注方式。于是，理论~\ref{theorem:1}得证。

\end{proof}

但是，列出所有可能的文本段组合方式是一个很不合实际的方式，因为，(1)因为未标注的文本段个数很多，用全排列会产生大量的组合结果，很多组合结果完全没有考虑\emph{锚点块}的位置，是完全没有意义的，这增加了整体标注错误的几率， (2)得到如此多组合结果，处理起来会花费大量的时间，这种方式效率上会大受影响。为了解决这些问题，我们提出了一种\emph{贪婪式概率标注算法}，这个算法基于每个文本段属于所有候选属性的概率值，然后寻找到一个对于当前输入文本的尽可能接近最佳的标注结果。在算法~\ref{alg:1}中详细介绍了这种贪婪式的标注算法：对于一个输入文本 $I$ ，另 $S(I)=\{s_1, s_2, ..., s_m\}$ 表示从文本 $I$ 中分割得到的所有文本段，最开始，一部分这些文本段被定义为\emph{锚点块}，对于剩下的未标注的文本段，我们以一种贪婪的模式来标注他们。首先，我们在输入文本中寻找有没有存在相邻的\emph{锚点块}，比如图~\ref{chap:chap03:fig:labelExample}(2)中的点块 $b_1b_2$ 和点块 $c_1$ ,如果找到了，我们便可以将输入文本分割成两部分，比如像图~\ref{chap:chap03:fig:labelExample}(3)中的这条分割线，这样我们就可以分开来执行标注算法，这可以提高标注效率。然后，对于每一个分区，基于CNN 模型的输出概率向量，使用公式~\ref{eq:2}，我们便可以计算出来每个文本块属于它的所有候选属性的概率值。在这些所有未标注的文本块中，每一次我们只贪婪地选择一个出来进行标注，记做$s_c$ ，这个选出来的文本段的概率应该是剩下这一分区中未标注文本段中概率值最高的那一个。然后，我们根据概率值为文本段 $s_c$ 标注上对应属性，记做 $A_c$，然后，我们立即检测是不是存在一个非邻接的文本段 $s'$ ，且这个文本段 $s'$ 也已经具有属性 $A_c$ ，如果出现这种情形，我们需要检查 $s'$ 和 $s_c$ 之间是不是已经存在标注了其他属性的文本段，如果，这种情况出现了，我们便取消这次对 $s_c$ 的标注，然后重新选择具有第二高概率值的文本段来进行相同的标注。如果这种情形没有发生，我们需要将 $s'$ 和 $s_c$ 之间所有的文本段合并形成一个新的文本段，且标注为 $A_c$ 。 我们重复执行这个过程，直到输入文本 $I$ 中没有可以被标注的文本段为止。注意，在这个过程中，我们定义了一个阈值 $\lambda$ 来防止产生低质量的标注，我们会在实验部分检测这个阈值的作用，并寻找到它的最佳值。

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.8\textwidth]{../figures/chap03/greedyLabelling.pdf}\\
  \smallcaption{贪婪式概率标注例子}
  \label{chap:chap03:fig:greedyLabelling}
\end{figure}


例子三：对于例子一种的输入文本，图~\ref{chap:chap03:fig:candidate-Labels}中给出了每个未标注文本段对于对应的候选属性的基于CNN模型的变换概率值，
我们设定阈值 $\lambda=0.3$，我们执行文本标注的算法~\ref{alg:1}如下：

\begin{enumerate}[1)]\setlength{\itemsep}{-1pt}
 \item 首先，我们像图~\ref{chap:chap03:fig:labelExample}(3)中一样将输入文本分成两个部分，然后独立的在每个分区上执行我们的标注算法。如图
 ~\ref{chap:chap03:fig:greedyLabelling}所示。

 \item 在第一个分区，我们首先标注 $a_1$ 为属性 $A$ 的一个文本块，因为，$s_1$ 在这个分区中的所有文本块中具有最高的概率值。然后，我们继续标注 $a_1$ 为属性 $A$ 的一个文本块，此时，我们完成了第一个分区的标注任务。

\item 在第二个分区有5个未标注的文本段，其中文本段 $c_3$ 属于属性 $C$ 具有最高的概率值，将被第一个标注。然后，我们通过标注$c_2$ 为属性 $C$ 来合并 $c_1$ 和 $c_3$。至此，我们还有三个未标注的文本段，其中，$c_4$ 具有最高的概率值首先被标注为 $E$ ，然后，$e_1$ 应该是一下个被标注为 $E$ 的，但是，因为它与 $c_4$ 冲突了，所以我们撤销了对 $e_1$ 的标注，然后标注文本块 $f_1$。直到最后 $e_1$ 任然不会被标注，因为它的概率值小于我们的阈值 ($\lambda=0.3$ )。

\end{enumerate}

使用我们的基于CNN的贪婪式概率标注算法，大部分输入文本都可以被正确的分割和标注了，只有一些特殊的情形中出现了错误标注（mis-matching）和漏标注（un-matching），比如像例子三中的 $c_4$ ， $e_1$ 。为了处理这种错误，在下一节中，我们从测试数据集中实时地学习到已经标注文本段的位置和序列信息，使用一个模型来权衡这些位置和序列信息来修正目前的标注结果。


\begin{algorithm}%[!htb]
    %\linesnumbered
    \large
    %\tiny
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
% \SetKwFunction{IntervalPrune}{\textbf{IntervalPrune}}
    \SetKwRepeat{doWhile}{do}{while}
    \SetKw{Return}{return}

    \Input{Input text $I$, segments $S(I)=\{s_1, s_2, ..., s_m\}$, $P=\{p(s_1, $ $A_{c1}), p(s_1, A_{c2}), ...\}$, where $p(s_i, A_{cj})$ denotes the probability that an segment $s_i \in S(I)$ belonging to its candidate attribute $A_{cj}$ according to Equation~\ref{eq:2}, and a minimum labelling probability threshold $\lambda$}
%    \Input{The probability if $s_i$ belonging to attribute $A_c$ in Equation 2 $p(s_i, A_c)$}
    \Output{Label set $\mathbf{A}(I) =\{ A_{l1}, A_{l2}, ... A_{lm}\}$ for segments in $S(I)$}
    \BlankLine

    Initialize $\mathbf{A}(I)$ by setting each $A_{li}=-1$, where $A_{li} \in \mathbf{A}(I)$\;
   $\mathbf{Pt}=\{Pt_1, Pt_2, ...\} \leftarrow$ Partitioning $S(I)$ into adjacent subsets\;

   \ForEach{$Pt \in \mathbf{Pt}$}{
   \doWhile{$P \neq \emptyset$ \&\& $\exists$ unlabelled segment in $Pt$}{
   	$(s_c, A_c) \leftarrow$ the pair with $\max_{s \in Pt, A \in \mathbf{A}_c(s)} p(s, A)$\;
	\If{$p(s_c, A_c) \leq \lambda$}{
		break\;
	}
	
   	$P = P - \{p(s_c, A_c)\}$\;
	$ind(s_c) \leftarrow$ the index of $s_c$ in $S(I)$\;
	\If{$A_c=A_{lq} \in \mathbf{A}(I)$}{
		\If{$\nexists A_{lr} \neq \mathbf{A}(I)$ between the $ind(s_c)$-th and $q$-th positions in $\mathbf{A}(I)$}{
			Set the $ind(s_c)$-th element in $\mathbf{A}(I)$ into $A_c$\;
			Also set all elements between the $ind(s_c)$-th and $q$-th positions in $\mathbf{A}(I)$ into $A_c$\;
		}		
	}
   }
   }
   \Return{$\mathbf{A}(I)$}\;

\caption{\small 基于CNN的贪婪式概率标注算法，CNN-based Greedy Probabilistic Labeling}
\label{alg:1}
\end{algorithm}


\subsection{基于序列和位置的结果修正}
\label{Sequential-based Amendment}

我们提出了一种双向的位置和序列模型来捕捉输入文本中已经标注了的文本段的位置和序列信息。对于一个输入文本$I$，假设有一组文本段 $S(I)=\{s_1, s_2, ..., s_m\}$ ，并且他们对应的标签为 $\mathbf{A}(I) =\{ A_{l1}, A_{l2}, ... A_{lm}\}$ 。对于一个文本段 $s_i$ ，他的两个相邻的文本段为 $s_{i-1}$ 和 $s_{i+1}$ ，这三个文本段分别被标注为属性 $A_{li}$, $A_{l(i-1)}$ and $A_{l(i+1)}$ 。序列模型考虑：（1）从$A_{l(i-1)}$ 转移到 $A_{li}$ 的概率以及从 $A_{li}$ 转移到 $A_{l(i-1)}$ 的概率。（2）属性 $A_{li}$ 出现在某个位置的概率。

因为经过我们的基于CNN的贪婪式概率标注算法得到的标注结果已经具有非常高的准确率（后面的实验中会证明这一点），所以，这个结果可以用来学习序列相关的特征和输入文本中属性值位置的信息。有一点很重要，这些特征的学习是从每一组输入文本中实时的（on-demand）学习到的，不需要人工的训练，也没有属性值遵循某一个特定的顺序这个假设，然后我们构建一种类似隐马尔科夫模型的图模型来强化此前得到的标注结果，我们称之为BPSM（Bidirectional Positioning and Sequencing Model）。相比较 ~\cite{cortez2010ondux}中提出的单相位置和序列模型(PSM)，我们的模型从双向考虑序列之间的关联，这可以减少单向标注中出错的风险。

\smallskip
\noindent \underline{\em 双向位置与序列模型(BPSM):} 我们的模型考虑：（1）对于一组状态 $L = \{begin, l_1, l_2, $ $..., l_n, end\}$ ，这里状态 $l_i$ 表示在我们的基于CNN的概率标注算法中每一个文本段被分配到的标签，$begin$ 和 $end$ 表示两个虚拟的状态，仅仅表示一个输入文本的开头和结尾，是我们人为添加上的。（2）矩阵 $FT$ 中存储了在标注结果中从状态 $l_i$ 转移到状态 $l_j$ 的概率，这是一种前向转移，表示从状态$begin$ 到 状态 $end$。（3）矩阵 $BT$ 中存储了在标注结果中从状态 $l_j$ 转移到状态 $l_i$的概率，是一种后向转移，表示从状态 $end$ 到状态 $begin$。（4）矩阵 $P$ 存储了在标注结果中输入文本中的一个标签为 $l_i$ 的文本段出现在 $pos$ 位置的概率。

我们构建矩阵 $FT$ ，需要考虑在基于CNN的标注结果中从状态 $l_i$ 到 状态 $l_j$ 的数量和从状态 $l_i$ 出发的前向的所有数量的比例。所以，概率矩阵 $FT$ 中每一个概率值 $ft_{i,j}$ 的定义方式如下：
\begin{equation}
ft_{i,j} = \frac{\#\; of\; transitions\; from\; l_i\; to\; l_j}{Total\; \#\; of\; transitions\; out\; of\; l_i}
\end{equation}
这里只是计算前向的转移，同理，我们从只考虑后向转移来构建概率矩阵 $BT$ ，$BT$ 中的每个概率值 $bt_{k, j}$ 的定义方式如下：
\begin{equation}
bt_{k, j} = \frac{\#\; of\; transitions\; from\; l_k\; to\; l_j}{Total\; \#\; of\; transitions\; out\; of\; l_k}
\end{equation}
这里只从后向来计算转移概率。

我们使用从CNN标注的结果中观察到的状态$l_j$出现在 $pos$ 位置的次数比上出现在位置 $pos$ 所有状态的总数目来构建矩阵 $PS$ 。因此，矩阵 $PS$ 中每个概率值的定义方式如下：
\begin{equation}
ps_{j,pos} = \frac{\#\;of\;observations\;of\;l_j\;in\;pos}{Total\;\#\;of\;segments\;in\;pos}
\end{equation}

根据CNN标注模型的标注结果，我们使用概率矩阵 $TF$ ， $BT$ ，$P$ 来最大化输入文本中属性值出现的序列和位置概率和，这种方式遵循了最大化概率方式（Maximum Likelihood approach），通常被用来训练图相关的模型~\cite{borkar2001automatic, sarawagi2008information}。在实际操作中，构建矩阵 $TF$ ， $BT$ ，$PS$  需要在CNN标注模型的输出结果上执行单趟的观察。这里注意，漏标注的文本段（mis-matching）在构建矩阵的时候是被自动忽略的，虽然构建矩阵时也可以加上这些漏标注文本段的数量，但这可能会刻画出谬误的转移。此外，在我们的实验中发现，CNN标注模块中产生的漏标注文本段的数量本身是非常少的，因此我们不必考虑这些漏标注的文本段，它们也不会对整体的修正起到很大的影响。

图~\ref{chap:chap03:fig:forwardpsm} 和 图~\ref{chap:chap03:fig:backwardpsm} 展示了我们使用CAR数据集构建的BPSM模型，我们可以发现，这个模型不仅可以表示分配给文本段的属性序列相关的信息，也可以表示输入文本中位置相关的信息。例如，在这个数据集中，输入文本开头的属性值，相比为‘Color’的属性值，更倾向于是为‘Brand’的属性值，还有，为‘Engine’的属性值出现在为‘Price’的属性值后面的概率会更大。这些信息都是对输入文本结构相关信息的描述。当构建好了BPSM模型，我们便可以用这些估计出来的概率值来优化标注结果。

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.7\textwidth]{../figures/chap03/forwardpsm.pdf}\\
  \smallcaption{CAR数据集上产生的BPSM例子-前向和位置}
  \label{chap:chap03:fig:forwardpsm}
\end{figure}

\begin{figure}[!h]
  \centering
  \includegraphics[width=0.7\textwidth]{../figures/chap03/backwardpsm.pdf}\\
  \smallcaption{CAR数据集上产生的BPSM例子-后向和位置}
  \label{chap:chap03:fig:backwardpsm}
\end{figure}


\smallskip
\noindent \underline{\em 使用BPSM来做修正:}
我们在执行最后修正的步骤时，同时考虑三个因素，序列概率，位置概率和CNN标注模型的结果，是非常重要的。考虑到这三个因素是相互独立的，文章~\cite{cortez2010ondux}中讨论了这个问题，一种 Noisy-OR-Gate模型 ~\cite{pearl2014probabilistic} 非常适合来整合这三个因素来决定每个文本段最终的标注方案。 因此，对于文本段 $s$ 属于属性 $A_j$ 的概率分别从前向考虑和后向考虑为：
\begin{equation}
%\scriptsize
P_{fw}(s, A_j) = 1-((1-p(s, A_j))(1-ft_{i, j})(1-ps_{j, pos(s)}) )
\end{equation}
\begin{equation}
%\scriptsize
P_{bw}(s, A_j) = 1-((1-p(s, A_j))(1-bt_{k, j})(1-ps_{j, pos(s)}) )
\end{equation}
这里，$s$ 表示在输入文本中位置为 $pos$ 的文本段，它前面是是一个被标注为 $A_j$ 属性的文本段，后面是一个被标注为 $A_k$ 属性的文本段。概率项 $ft_{i,j}$, $bt_{k,j}$ 和 $ps_{j,pos(s)}$ 分别来自矩阵 $TF$, $TB$ and $PS$。

对于输入文本 $I$ 的一组文本段 $S(I)=\{s_1, s_2, ..., s_m\}$ ，在一次迭代中，我们需要比较输入文本中最前面的一个未标注文本段的前向概率，记作 $P_{fw}(s_1, A_{l1})$ ，和最后一个未标注文本段的后向概率，记作 $P_{bw}(s_m, A_{lm})$ 。如果 $P_{fw}(s_1, A_{l1}) \geq P_{bw}(s_m, A_{lm})$ ，我们首先标注 $s_1$，然后继续比较 $P_{fw}(s_2, A_{l2}) $ 和 $P_{bw}(s_m, A_{lm})$ 。否则，我们首先标注 $s_m$ ，然后继续比较 $P_{fw}(s_1, A_{l1}) $ 和 $P_{bw}(s_{m-1}, A_{l,m-1})$  来决定哪一个是下一个被标注的文本段。我们希望通过这种思路，同时从前向和后向考虑，找到最佳的标注结果。我们循环执行这行这个过程，直到一条输入文本中所有的文本段都被标注了为止。


\section{实验结果与分析}
在这一节，我们会介绍本文提出的模型在在三个数据集上的实验结果，并做充分的分析和比较。

\subsection{数据集和评价标准}
我们在下面这三个真实数据集上做了实验：

\vspace{-2pt}
\begin{enumerate}[1)]\setlength{\itemsep}{-1pt}

  \item  \textit{引文数据集:} 我们从超过500个计算机领域的研究员的主页上手机了超过2.5万条的论文引文信息，这些引文信息中隐含的属性值属于{\em Title, Author, Journal, Volume, Pages,} and {\em Date}这些属性，文章~\cite{cortez2010ondux,zhao2008exploiting}中也使用了相同的数据集。另外，我们随机的从DBLP中选择了若干引文记录来构建我们的知识库(KB)。

  \item \textit{二手房:} 我们从Anjuke~\footnote{www.anjuke.com}, Ganji~\footnote{www.ganji.com} and Lianjia~\footnote{www.lianjia.com}上收集了超过10万的关于二手房出售和租赁相关信息的描述文本，每一条文本中包含的值属于属性：{\em Name, Publish-Time, Price, Charge-Method, Neighborhood, Unit, Floor,} and {\em Furnitures}。我们的知识库(KB)的构建使用相同来源的结构化的表格构建的。

  \item \textit{二手车:} 二手车的数据集，我们从Carsales~\footnote{www.carsales.com}网站中收集了超过20万条的关于二手车售卖的描述信息，这个数据集的属性集包括：{\em Brand, Price, Transmission, Color, Body, Engine, Fuel Consuming} ，我们的构建知识库(KB)的数据同样可以从这个网站上的结构化表格区域得到。

\end{enumerate}


\noindent{\bf 评价标注:}
我们使用准确率({\bf Precision})，召回率({\bf Recall})和F值({\bf F-Measure})来评测我们提出的模型。对于一个属性 $A$， 另 $N_l(A)$ 表示使用某个方法标注文本段的总数量，$N_c(A)$ 表示对于这个属性正确标注的文本段数量，$N_g(A)$ 表示测试集中应该被标注为属性 $A$的文本段数量。然后，在属性 $A$上的准确率可以表示成：$prec = \frac{N_c(A)}{N_l(A)}$，在属性 $A$上的召回率可以表示成：$recall =\frac{N_c(A)}{N_g(A)}$，在属性 $A$上的F值可以表示成：$F_1= \frac{2(prec \times recall)}{prec+recall}$ 。


\subsection{参数的设定}
在我们的方法中有两个重要的参数：一个是在章节~\ref{KB-based Segmentation and Labelling}中用来定义锚点快的阈值 $\theta$，另外一个是章节~\ref{Probabilistic Labelling to Input Texts}中算法一中用来防止低质量标注的阈值 $\lambda$ 。在我们的实验中，我们采用单一变量分析的方法，先设置阈值$\lambda=0.6$，然后在三个数据集上测试我们的方法中阈值 $\theta$ 对抽取质量和效率的影响。然后，设置阈值$\theta=0.9$，来测试阈值 $\lambda$ 对抽取质量的影响。注意，在这两组实验中，我们的知识库(KB)在测试集中的属性值覆盖率均不超过30\%。

在我们的方法中，图~\ref{fig:7}中表明了阈值 $\theta$ 对抽取质量和效率的影响。详细来讲，从图~\ref{fig:7}(a)中可以看出，当 $\theta$ 从 $0.4$ 增加到 $0.9$，在引文数据集中，F值首先从$0.880$ 平滑的增加到 $0.929$，在这点之后，当 $\theta$ 从 $0.9$ 增加到 $1.0$ ，F值从 $0.929$ 缓慢的下降到 $0.910$ 附近。我们在图~\ref{fig:7}(b)和图~\ref{fig:7}(c)中的二手房和二手车数据集中可以看到相同的趋势。 基于上面的实验结果，我们得出结论，在所有这三个数据集中，来定义锚点快时，设置阈值 $\theta = 0.9$ 是一个合适的选择，这个值也将在后面的实验部分中来使用。在另一方面，当考虑抽取效率时，我们从图~\ref{fig:7}(d)中可以发现，在引文数据集中，当 $\theta$ 从 $0.4$ 增加到 $1.0$ 时，抽取每条文本所花费的时间从 大概 $0.05$秒 缓慢增加到 $0.31$ 秒 。 在图~\ref{fig:7}(e)和图~\ref{fig:7}(f)中的二手房和二手车数据集中可以看到相同的趋势。因此，虽然 $\theta=0.9$ 可能不能实现最佳的抽取效率，但是牺牲一点点效率来换取更佳的抽取质量是可以接受的。

图~\ref{fig:8}中表明了阈值 $\lambda$ 对抽取质量的影响。从图~\ref{fig:8}(a)中可以发现，当阈值 $\lambda$ 的值从 $0.2$ 增加到 $0.6$ ，引文数据集上的抽取质量首先从 $0.817$ 增长到 $0.929$ ，在 $\lambda=0.6$ 时达到最大值。当阈值 $\lambda$ 的值从 $0.6$ 增加到 $1.0$ ，我们方法的抽取质量从  $0.929$ 缓慢的下降到 $0.808$ 。在另外两个数据集中，如图 ~\ref{fig:8}(b) 和图 ~\ref{fig:8}(c)所示，可以看到相同的现象。因此，我们可以得出结论，算法一中在过滤一些糟糕的标注的环节，$\lambda=0.6$ 或许是一个合适的值，我们也将在下面的实验中使用这个值。


\begin{figure*}[!ht]
\centering
%mobile
\subfigure[F-Measure on PersonalBib]{
    \label{fig:subfig:a}
    \begin{minipage}[h]{0.6\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../figures/chap03/Exp/bib_f.pdf}
    \end{minipage}
    }
\subfigure[F-Measure on House]{
    \label{fig:subfig:c}
    \begin{minipage}[h]{0.6\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../figures/chap03/Exp/house_f.pdf}
    \end{minipage}
    }
\subfigure[F-Measure on Car]{
    \label{fig:subfig:c}
    \begin{minipage}[h]{0.6\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../figures/chap03/Exp/car_f.pdf}
    \end{minipage}
    }
\end{figure*}


\begin{figure*}[!ht]
\centering
\subfigure[Time Cost on PersonalBib]{
    \label{fig:subfig:a}
    \begin{minipage}[h]{0.6\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../figures/chap03/Exp/bib_e.pdf}
    \end{minipage}
    }
\subfigure[Time Cost on House]{
    \label{fig:subfig:c}
    \begin{minipage}[h]{0.6\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../figures/chap03/Exp/house_e.pdf}
    \end{minipage}
    }
\subfigure[Time Cost on Car]{
    \label{fig:subfig:c}
    \begin{minipage}[h]{0.6\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../figures/chap03/Exp/car_e.pdf}
    \end{minipage}
    }
\caption{Evaluating the Effect of Threshold $\theta$ to the Extraction Quality and Efficiency ($\lambda = 0.6$)}
\label{fig:7}
\end{figure*}


\begin{figure*}[!ht]
\centering
%mobile
\subfigure[F-Measure on PersonalBib]{
    \label{fig:subfig:a}
    \begin{minipage}[h]{0.6\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../figures/chap03/Exp/bib_lambda.pdf}
    \end{minipage}
    }
\subfigure[F-Measure on House]{
    \label{fig:subfig:c}
    \begin{minipage}[h]{0.6\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../figures/chap03/Exp/house_lambda.pdf}
    \end{minipage}
    }
\subfigure[F-Measure on Car]{
    \label{fig:subfig:c}
    \begin{minipage}[h]{0.6\textwidth}
    \centering
    \includegraphics[width=\textwidth]{../figures/chap03/Exp/car_lambda.pdf}
    \end{minipage}
    }
\caption{Evaluating the Effect of Threshold $\lambda$ to the Extraction Quality ($\theta = 0.9$)}
\label{fig:8}
\end{figure*}


\subsection{与其他方法的比较}
我们比较了我们的方法(CNN-IETS)的抽取质量与另外两种目前表现最佳的方法的抽取质量，为了公平的比较，对于每个数据集上的每个方法，我们都将他们调整成具有最佳的抽取表现。

\vspace{-2pt}
\begin{enumerate}[1)]\setlength{\itemsep}{-1pt}

\item 基于CRF的监督式模型 ({\bf CRF}): 这是目前表现最佳的一种基于隐马尔科夫模型(CRF)
的监督式信息抽取方法~\cite{lafferty2001conditional}，我们构建CRF模型所使用的特征包括文本自身特征，模型布局特征，外部词典源，文章~\cite{peng2006information}中有信息的说明。这些特征也被看做是一种标准的特征。

\item 非监督式ONDUX方法 (\noindent {\bf ONDUX}): ONDUX~\cite{cortez2010ondux} 是目前分割式信息抽取任务中采用非监督式方法表现最佳的一种方法。这种方法首先将根据一个知识库，将输入文本粗略的分割成若干个文本块，每一个文本块是可能组成一个属性值的一组词语，然后通过一组特定的匹配函数，将这些文本块与已知的属性进行匹配。为了比较的公平，这种ONDUX方法与我们的方法(CNN-IETS)使用同一个知识库(KB)。
\end{enumerate}


\begin{table}%[!hb]
\centering
\large
%\caption{Comparing the Extraction Quality on PersonalBib}
\topcaption{Comparing the Extraction Quality on PersonalBib}
\label{table:1}
\begin{tabular}{| l | c | c | c |}
\hline
Attribute & CRF & ONDUX & CNN-IETS \\ \hline
Title & 0.829 & 0.781 & {\bf 0.833} \\ \hline
Author & 0.856 & {\bf 0.861}  & 0.859 \\ \hline
Journal & 0.772 & 0.803 & {\bf 0.932} \\ \hline
Volume & 0.787 & 0.822 &  {\bf 0.987} \\ \hline
Pages & 0.832 & 0.831 & {\bf 0.979} \\ \hline
Date & 0.782 & 0.817 &  {\bf 0.998} \\ \hline
{\bf Average} & 0.809 & 0.810 &  {\bf 0.929}  \\ \hline
%Whole-instance & 0.693 & 0.721 & 0.736 \\ \hline
\end{tabular}
\end{table}

\begin{table}%[!hb]
\centering
\large
%\caption{Comparing the Extraction Quality on House}
\topcaption{Comparing the Extraction Quality on House}
\label{table:2}
\begin{tabular}{| l | c | c | c |}
\hline
Attribute & CRF & ONDUX & CNN-IETS \\ \hline
Name  & 0.894 & 0.867 &  {\bf 0.977} \\ \hline
Publish-time & 0.829 & 0.880 & {\bf 0.985} \\ \hline
Price  & 0.846 & 0.877 &  {\bf 0.985}  \\ \hline
Charge-Method & 0.859 & 0.907 &  {\bf 0.989} \\ \hline
Neighborhood & 0.837 & 0.875 &  {\bf 0.984} \\ \hline
Unit & 0.835 & 0.880 &  {\bf 0.987} \\ \hline
Floor & 0.847 &  0.894 & {\bf 0.985} \\ \hline
Furnitures & 0.821 & 0.871 &  {\bf 0.996} \\ \hline
{\bf Average} & 0.846 & 0.845 & {\bf 0.986} \\ \hline
%Whole-instance & 0.879 & 0.921 & {\bf 0.951} \\ \hline
\end{tabular}
\end{table}


\begin{table}%[!hbp]
\centering
\large
%\caption{Comparing the Extraction Quality on Car}
\topcaption{Comparing the Extraction Quality on Car}
\label{table:3}
\begin{tabular}{| l | c | c | c |}
\hline
Attribute & CRF & ONDUX & CNN-IETS \\ \hline
Brand & 0.844 & 0.857 &  {\bf 0.972} \\ \hline
Price & 0.794 & 0.890 & {\bf 1.000} \\ \hline
Transmission & 0.825 & 0.872 & {\bf 1.000} \\ \hline
Color & 0.773 & 0.858 & {\bf 0.868} \\ \hline
Body & 0.769 & 0.866 &  {\bf 1.000}  \\ \hline
Engine & 0.801 & 0.830 &  {\bf 1.000}  \\ \hline
Fuel-consuming & 0.813 & 0.854 & {\bf 0.979}  \\ \hline
{\bf Average} & 0.803 & 0.861 & {\bf 0.968} \\ \hline
%Whole-instance & 0.721 & 0.762 & 0.847 \\ \hline
\end{tabular}
\end{table}



\noindent {\bf 抽取质量的比较}:
我们在三个数据集中比较了这三种方法的抽取质量，为了全面的比较，我们不仅比较各个属性的F值得分，也比较了在每个数据集上所有属性的平均F值得分。我们从表~\ref{table:1}，表~\ref{table:2} 和表~\ref{table:3}中可以看到，我们的方法在三个数据集的几乎所有属性上的表现都好于另外两种方法。整体来讲，我们的方法(CNN-IETS)相比较另外两种目前表现最好的方法(CRF， ONDUX)，在三个数据集上，抽取质量平均提升了超过10\%。

\noindent {\bf 抽取效率的比较}:
我们另外比较了使用这三种方法来抽取一条文本所花费的平均时间，从表~\ref{table:4}中可以看到，相比较ONDUX和CNN-IETS，基于CRF的方法花费了更多的时间来训练自己的模型。尽管，我们的方法(CNN-IETS)也需要训练CNN模型，但是这个训练的过程可以使用已经存在的知识库在线下来运行，因此我们的方法跟ONDEX方法在做线上的信息抽取时所花费的时间差不多。

\begin{table}%[!hbp]
\centering
\large
%\caption{三种方法花费时间的比较}
\topcaption{三种方法花费时间的比较}
\label{table:5}
\begin{tabular}{| l | c | c | c |}
\hline
Dataset & CRF & ONDUX & CNN-IETS \\ \hline
PersonalBib & 2350ms & 190ms &  150ms \\ \hline
House & 4670ms & 380ms & 340ms \\ \hline
Car & 3780ms & 200ms & 210ms \\ \hline
\end{tabular}
\label{table:4}
\end{table}


\noindent {\bf 知识库(KB)的覆盖率对抽取质量的比较}:
因为ONDUX方法和我们的方法都需要使用一个事先构建好的知识库用来理解输入文本的结构特征，因此，知识库在测试集中属性值的覆盖率对抽取质量有非常大的影响。经过试验，我们得出结论，相比较ONDEX的方法，我们的CNN-IETS方法对知识库覆盖率的要求更低。从图~\ref{fig:9}(a)中可以看到，当知识库的覆盖率从 $0.1$ 增加到 $0.6$ ，在引文数据集上，CNN-IETS方法的抽取质量从 $0.874$ 缓慢地增加到 $0.929$ ，而ONDEX方法的抽取质量急剧的从 $0.704$ 增加到 $0.810$ 。另外，不管知识库的覆盖率是多少，我们的CNN-IETS方法的表现都要好于ONDEX方法。从图~\ref{fig:9}(b)和图~\ref{fig:9}(c)中我们可以发现相同的现象，这就证明了，我们的方法相比较ONDEX方法对知识库覆盖率的依赖更低。


\subsection{BPSM效果的验证}
我们提出了一个全新的类似隐马尔科夫的概率图模型，双向位置与序列模型(BPSM)，用来进一步修正CNN模型得到的标注结果，相比较在ONDEX方法~\cite{cortez2010ondux}中使用的单向位置和序列模型(PSM)，我们的模型同时从前向和后向来考虑序列关系，因此可以有效的避免使用单向模型可能出现标注错误的风险。

为了验证BPSM的有效性，已经相比较PSM的优势，我们在三个数据集上做了实验，比较了三种模型下的抽取质量，这三种模式分别是：纯粹基于卷积神经网络的方法(CNN)，卷积神经网络结合单向位置与序列模型的方法(CNN + PSM)，卷积神经网络结合双向位置与序列模型的方法(CNN + BPSM)。从表~\ref{table:5}中可以看到，CNN + PSM的方法在所有三个数据集上均取得了最佳的F值。具体来讲，使用CNN+PSM仅仅比单纯使用CNN的抽取质量在三个数据集上平均提升了大概3-5\%，但是，使用CNN+BPSM比单纯使用CNN的抽取质量在三个数据集上平均提升了大概5-8\%。因此，我们得出结论，我们提出的双向位置与序列模型确实好过单向的位置与序列模型。


\begin{table}%[!hbp]
\centering
\large
%\caption{\small  CNN, CNN+PSM 和 CNN+BPSM 三种模型抽取质量的比较}
\topcaption{ CNN, CNN+PSM 和 CNN+BPSM 三种模型抽取质量的比较}
\label{table:5}
\begin{tabular}{| c | c | c | c |}
\hline
Dataset & CNN  & CNN + PSM & CNN + BPSM \\ \hline
PersonalBib & 0.862 & 0.891 &  {\bf 0.929} \\ \hline
House & 0.903 & 0.943 & {\bf 0.986} \\ \hline
Car & 0.897 & 0.932 & {\bf 0.968} \\ \hline
\end{tabular}
\end{table}


\section{本章小结}

本章，我们给出了分割式信息抽取(IETS)的正式定义，介绍了知识库的构建，并详细介绍了本文提出的基于卷积神经网络的贪婪式概率标注算法的每一部分，最后我们在三个数据集上与另外两种目前表现最佳的方法做了充分的对比实验，给出了详细的实验结果和分析，最终得到结论，文本提出的方法不仅在抽取质量还是在抽取效率上都明显好于其他方法。


